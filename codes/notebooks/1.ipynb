{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the input embeddings class : \n",
    "- takes up 2 inputs : \n",
    "    - number of words i.e `vocab_size`\n",
    "    - dimentions of each word embedding i.e `d_model`\n",
    "- this is used to create an embedding object present in `torch.nn` library. \n",
    "- can refer to Documentation of [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "- `torch.nn.Embedding(num_embeddings, embedding_dim)` : \n",
    "    - `num_embeddings` = total no. of embeddings needed to store \n",
    "    - `embedding_dim`  = the dimensions we want for each word embedding\n",
    "- the forward method has this input x that's nothing but the index of the word whose embedding is required. \n",
    "- **NOTE** that the embedding that we return is multiplied by ${\\sqrt{d_{model}}}$. This is discussed in the section `3.4` of the paper [Attention is all you Need](https://arxiv.org/pdf/1706.03762)\n",
    "##### TODO : Checkout for the reason for this step of multiplying by ${\\sqrt{d_{model}}}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List,OrderedDict,Dict,Tuple,Literal\n",
    "from math import sqrt,pow,sin,cos\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.vocab_size,\n",
    "                                       embedding_dim=self.vocab_size)\n",
    "    def forward(self,x):\n",
    "        return self.embeddings(x)*sqrt(self.d_model)\n",
    "    # TODO : Find the reason for mutiplying with the sqrt(d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding:\n",
    "<div align=\"center\">\n",
    "\n",
    "|Good |morning| to| all |of |you|\n",
    "|----|----|----|----|----|----|\n",
    "|120|125|156|2145|215|7653|\n",
    "|20|1345|156|2145|215|7653|\n",
    "|10|145|156|2145|215|7653|\n",
    "|11|345|156|2145|215|7653|\n",
    "|10|135|156|2145|215|7653|\n",
    "|1|12|156|2145|215|7653|\n",
    "|2|345|156|2145|215|7653|\n",
    "|....|.....|....|....|....|....|\n",
    "|....|.....|....|....|....|....|\n",
    "|....|.....|....|....|....|....|\n",
    "|....|.....|....|....|....|....|\n",
    "|....|.....|....|....|....|....|\n",
    "\n",
    "</div>\n",
    "\n",
    "- Each word in the vocabulary will have embedding vectors for them.\n",
    "- Each vector will be `d_model` long. \n",
    "- We want to help the model to get an idea of what has come before the current word. \n",
    "- Further more we want the word to carry some information about the position and it's relative importance in the sentence \n",
    "- One way to do this is via **Positional Encoding**\n",
    "- We add another vector : the positional encoding vector to it.\n",
    "- ${\\textbf{PE}(x,2y  )=\\sin({\\frac{x}{1000^{\\frac{2y  }{d_{model}}}}}) \\quad \\forall\\textbf{ embedding values at even positions in a word embedding}  }$ \n",
    "- ${\\textbf{PE}(x,2y+1)=\\cos({\\frac{x}{1000^{\\frac{2y  }{d_{model}}}}}) \\quad\t\\forall\\textbf{ embedding values at odd  positions in a word embedding}}$ \n",
    "- here $x$ is the index of word in the sentence and $y$ supposedly denotes the index of embedding. \n",
    "\n",
    "\n",
    "|Good |morning| to| all |of |you|\n",
    "|----|----|----|----|----|----|\n",
    "|${\\textbf{PE(0,0)}}$|${\\textbf{PE(1,0)}}$|${\\textbf{PE(2,1)}}$|${\\textbf{PE(3,0)}}$|${\\textbf{PE(4,0)}}$|${\\textbf{PE(5,0)}}$|\n",
    "|${\\textbf{PE(0,1)}}$|${\\textbf{PE(1,1)}}$|${\\textbf{PE(2,1)}}$|${\\textbf{PE(3,1)}}$|${\\textbf{PE(4,1)}}$|${\\textbf{PE(5,1)}}$|\n",
    "|${\\textbf{PE(0,2)}}$|${\\textbf{PE(1,2)}}$|${\\textbf{PE(2,2)}}$|${\\textbf{PE(3,2)}}$|${\\textbf{PE(4,2)}}$|${\\textbf{PE(5,2)}}$|\n",
    "|${\\textbf{PE(0,3)}}$|${\\textbf{PE(1,3)}}$|${\\textbf{PE(2,3)}}$|${\\textbf{PE(3,3)}}$|${\\textbf{PE(4,3)}}$|${\\textbf{PE(5,3)}}$|\n",
    "|${\\textbf{PE(0,4)}}$|${\\textbf{PE(1,4)}}$|${\\textbf{PE(2,4)}}$|${\\textbf{PE(3,4)}}$|${\\textbf{PE(4,4)}}$|${\\textbf{PE(5,4)}}$|\n",
    "|....|.....|....|....|....|....|\n",
    "|....|.....|....|....|....|....|\n",
    "|upto ${\\textbf{PE}(0,d_{model})}$ |upto ${\\textbf{PE}(1,d_{model})}$|upto ${\\textbf{PE}(2,d_{model})}$|....|....|....|\n",
    "\n",
    "\n",
    "- This formula was given in the section `3.5` of the paper : [Attention is all you need](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "- A really nice article to understand as to why we need this very formula for postional encodings : [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "\n",
    "- A really nice video for the same : [Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Personal understanding: \n",
    "- Transformers are encoder-decoder stacks. \n",
    "- They don't have any understanding of words as such nor their position or order.  \n",
    "- Hence we need to incorporate this understanding via some means. \n",
    "- One possible way could be by adding some clues or some pieces of information in the inputs itself.\n",
    "- This information is mainly about the position of the word in the sequence.\n",
    "- So we now need to figure out any method or trick using which we can pass the positional information of a word to the model. \n",
    "\n",
    "-----\n",
    "### Idea 1: \n",
    "One possible solution would be to assign numbers index wise like:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|I|love|to|pet|cats|\n",
    "|----|----|----|----|----|\n",
    "|$1$| $2$  | $3$|$4$  | $5$   |\n",
    "\n",
    "</div>\n",
    "\n",
    "the **issue** here is that as we go for longer sentences, the words coming later will have higher weightage which is not a good direction to generalize the model for learning the sequential information. \n",
    "\n",
    "### Idea 2: \n",
    "As the size of sentence increases, the importance distributes. Something like: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "| I    | love |  to  | play |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| $0.00$ | $0.33$ | $0.66$ | $1.00$ |\n",
    "\n",
    "</div>\n",
    "\n",
    "- We had to assign $4$ numbers in between $0$ and $1$. So we skipped the $0^{th}$ one and checked 1 can be divided into how many pieces. $3$ pieces remain, so we divide $1$ into $3$ pieces with each piece worth $0.33$. Therefore\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "$I \\rightarrow 0.00$  \n",
    "$love\\rightarrow 0.33$  \n",
    "$to\\rightarrow0.66$  \n",
    "$play\\rightarrow 1.00$ \n",
    "\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| I    | love |  to  | play | it   |\n",
    "| ---- | ---- | ---- | ---- | ---- |\n",
    "| $0.00$ | $0.25$ | $0.50$ | $0.75$ | $1.00$ |\n",
    "\n",
    "</div>\n",
    "\n",
    "- Extending the same logic for $7-1 = 6$ words: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| I    | love |  to  | play | it   | very | much |  \n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- |  \n",
    "| $0.00$ | $0.16$ | $0.33$ | $0.50$ | $0.66$ | $0.83$ | $1.00$ | \n",
    "\n",
    "</div>\n",
    "\n",
    "The **Issue** here is that the meaning conveyed here will vary drastically as we change the length of the sentence.\n",
    "- Suppose we have a $5$ length sentence and a $10$ length sentence. The difference of $0.5$ means a gap of $1$ word in the 5 word sentence whereas the difference of $0.5$ would mean a difference of $5$ words in a $10$ word sentence. Therefore the meaning conveyed by the values is highly varied. \n",
    "\n",
    "- In a $5$ length word, the value of $0.5$ conveys the positional value of $ 3^{rd} $ word in the sequence. But the same $0.5$ in a $10$ length word will convery the positional value of $5^{th}$ word in the sequence. \n",
    "\n",
    "Since our old $2$ ideas failed, we are now clear with the notion of what we want:\n",
    "- It should output a unique encoding for each position in the sentence. \n",
    "- Distance between any $2$ positions should be consistent through out the sentences even with different length. \n",
    "- Our model should be able to learn the positional significance of the values without much difficulty and in a generalizable way.\n",
    "\n",
    "------\n",
    "Here a point to **note** that we are using a **vector to represent a word rather a single number.** Hence we need to devise a mechanism to actually induce a **positional sense in vector that represents a words in a sequence**. \n",
    "Furthermore, this will **not become the part of the model's input parameters**, rather it will help the model to tune it's parameters. Hence we're **improving the input's interpretability for the model** to learn better.\n",
    "\n",
    "------\n",
    "So we need to define a function that takes a **vector index** and a **position** as an input, and returns the **value that will be stored in that vector for that vector index** for the output. \n",
    "So therefore, the input is : \n",
    "- ***position $t$*** : a number that signifies the location of the word in the sentence. \n",
    "- ***vector index $i$*** : a number that signifies a specific dimension in the embedding vector ranging from $[0,d_{model}-1]$ (considering $0$ based indexing)\n",
    "\n",
    "Let $ \\overrightarrow{p^{t}} \\in \\mathbb{R^{d_{model}}} $ be the final vector outout for the position $t$ in the sentence. \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "$\\implies\\overrightarrow{p^{t}_{i}} = f(t)^{(i)} = \\begin{Bmatrix}\n",
    "\\sin{(\\omega_{k} t)} \\textbf{  }\\textbf{ if }\\textbf{  } i = 2k; \\\\\n",
    "\\\\\n",
    "\\cos{(\\omega_{k} t)} \\textbf{  }\\textbf{ if }\\textbf{  } i = 2k+1\n",
    "\\end{Bmatrix}\n",
    "\\newline\n",
    "where : \\omega_{k} = \\frac{1}{1000^{\\frac{2k}{d_{model}}}}\n",
    "\\newline\n",
    "\\newline \n",
    "\\implies\\overrightarrow{p^{t}_{i}} = f(t)^{(i)} = \\begin{Bmatrix}\n",
    "\\sin{(\\frac{t}{1000^{\\frac{i}{d_{model}}}})} \\textbf{  }\\textbf{ if }\\textbf{  } i = 2k; \\\\\n",
    "\\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{i-1}{d_{model}}}})} \\textbf{  }\\textbf{ if }\\textbf{  } i = 2k+1\n",
    "\\end{Bmatrix}\n",
    "\\newline \n",
    "\\textbf{ final vector will look like : }\\newline \\begin{pmatrix}\n",
    "\\sin{(\\omega_{0} t)}  \\textbf{ for } i = 0 \\\\\n",
    "\\cos{(\\omega_{0} t)}  \\textbf{ for } i = 1 \\\\\n",
    "\\sin{(\\omega_{1} t)}  \\textbf{ for } i = 2 \\\\\n",
    "\\cos{(\\omega_{1} t)}  \\textbf{ for } i = 3 \\\\\n",
    "\\sin{(\\omega_{2} t)}  \\textbf{ for } i = 4 \\\\\n",
    "\\cos{(\\omega_{2} t)}  \\textbf{ for } i = 5 \\\\\n",
    "\\sin{(\\omega_{3} t)}  \\textbf{ for } i = 6 \\\\\n",
    "\\cos{(\\omega_{3} t)}  \\textbf{ for } i = 7 \\\\\n",
    "\\vdots \\\\ \n",
    "\\sin{(\\omega_{d_{model}-1} t)}  \\textbf{ for } i = d_{model}-2 \\\\\n",
    "\\cos{(\\omega_{d_{model}-1} t)}  \\textbf{ for } i = d_{model}-1 \n",
    "\\end{pmatrix} \\textbf{which is also :}  \\begin{pmatrix}\n",
    "\\sin{(\\frac{t}{1000^{\\frac{0}{d_{model}}}})}  \\textbf{ for } i = 0  \\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{0}{d_{model}}}})}  \\textbf{ for } i = 1  \\\\\n",
    "\\sin{(\\frac{t}{1000^{\\frac{1}{d_{model}}}})}  \\textbf{ for } i = 2  \\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{1}{d_{model}}}})}  \\textbf{ for } i = 3  \\\\\n",
    "\\sin{(\\frac{t}{1000^{\\frac{2}{d_{model}}}})}  \\textbf{ for } i = 4  \\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{2}{d_{model}}}})}  \\textbf{ for } i = 5  \\\\\n",
    "\\sin{(\\frac{t}{1000^{\\frac{3}{d_{model}}}})}  \\textbf{ for } i = 6  \\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{3}{d_{model}}}})}  \\textbf{ for } i = 7  \\\\\n",
    "\\vdots \\\\ \n",
    "\\sin{(\\frac{t}{1000^{\\frac{d_{model}-1}{d_{model}}}})}  \\textbf{ for } i = d_{model}-2 \\\\\n",
    "\\cos{(\\frac{t}{1000^{\\frac{d_{model}-1}{d_{model}}}})}  \\textbf{ for } i = d_{model}-1 \n",
    "\\end{pmatrix}$\n",
    "\n",
    "</div>\n",
    "\n",
    "Now since $d_{model}$ is fixed for a given case, thereby for the change in $i$,$\\newline$\n",
    "as $i$ increases ($k$ increases), the frequency $\\omega_{k}$ decreases.\n",
    "### TODO : I'll dig deeper into this later. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Postional Encoding Class info :\n",
    "- **d_model** : number of dimensions we need in a word embedding.\n",
    "- **sequence length** : length of input sequence ( basically number of words in input sentence )\n",
    "- **p_dropout** : The dropout-regularization probability. Dropout regularization has been used as the method of regularization while summing the embeddings and positional embeddings as explained in the section `5.4` of the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762) . \n",
    "- **Note** that the shape of the position embeddings is $({\\textbf{sequence length} \\times \\textbf{d\\_{model}}}) $\n",
    "- The formula is implemented in the same way as described above. \n",
    "-----\n",
    "```python\n",
    "        self.positional_embeddings = self.positional_embeddings.unsqueeze(0)\n",
    "```\n",
    "- Reason for doing this : this adds an extra dimension in the positional embeddings structure for enabling batching. This means the new dimensions of this positional_embeddings is  $ (1 \\times \\textbf{sequence length} \\times \\textbf{d\\_{model}}) $\n",
    "------\n",
    "```python\n",
    "        self.register_buffer(name='positional_embeddings',tensor=self.positional_embeddings)\n",
    "```\n",
    "- This line is used when we want to include the parameters into the `state_dict` of the model though we don't intend to train it. \n",
    "- What was the need ? Simply because for real-life cases where d_model is $512$ and sequence length becomes $>1000$, at that point recompting this matrix is extremely costly and time taking. So it's better to keep it pre computed somewhere. \n",
    "- **Note**: You can read the buffer stores spearately using the following code : \n",
    "```python\n",
    "         model = MyCustomModel() # suppose you have your own model class where you defined your buffer\n",
    "         for buffer_name, buffer in model.named_buffers():\n",
    "             print(f\"Buffer '{buffer_name}': {buffer}\")\n",
    "```\n",
    "\n",
    "- Reference Documentation : \n",
    "\n",
    "    - [torch.nn.Module.register_buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)\n",
    "    - [torch.nn.Module.get_buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_buffer) $\\rightarrow$ this one has no code for understanding in the docs, so try and test. \n",
    "-----\n",
    "```python\n",
    "def forward(self,sequence:torch.Tensor):\n",
    "    if sequence.ndim==2:\n",
    "        sequence =sequence.unsqueeze(0)\n",
    "    assert sequence.shape[2] == self.d_model,f\"\"\"The embedding dimensions of model and input dont match model's dimensions : {self.d_model} , input dimensions : {sequence.shape[0]}\"\"\"\n",
    "    sequence = sequence + (self.positional_embeddings[:,:sequence.shape[1],:]).requires_grad_(False)\n",
    "    return self.dropout(sequence)\n",
    "```\n",
    "- Here, the input is a sequence of words but we're only concerned about the shape of the input sequence. The reason is that we only need the positional values of the embeddings and nothing related to the word in reality. \n",
    "\n",
    "- assertion for equality `sequence.shape[2]` and `d_model` is necessary as they show the number of respective dimensions.  \n",
    "\n",
    "- taking up `sequence.shape[1]` tells that we are taking the sequence length of the input only. \n",
    "\n",
    "- `self.positional_embeddings[:,:sequence.shape[1],:].requires_grad_(False)`: here setting `requires_grad(False)` was important because we aren't supposed to learn these positional embeddings.\n",
    "\n",
    "\n",
    "- Furthermore, we're expecting the inputs to in the batched form as well i.e they also should have an extra dimension. To be precise, the following dimensions : $\\newline$\n",
    "`[n_batches,sequence_length,d_model]`. \n",
    "- If we don't get the input in this format , then we make the input in this format using the following code : \n",
    "```python \n",
    "if sequence.ndim==2:\n",
    "    sequence =sequence.unsqueeze(0)\n",
    "```\n",
    "\n",
    "- **Note** that we add the values of the positional encodings to the input sequence, i.e the input to the model is :\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$\\newline \\phi(x) + PE(x) \\newline \\textbf{ where } \\phi(x) \\textbf{ is the word embedding matrix of the sentence x}\\newline \\textbf{ where }\\phi(x) \\textbf{ looks like : } $\n",
    "\n",
    "\n",
    "\n",
    "|    | $d_{0}$   | $d_{1}$   | $d_{2}$|$d_{3}$| $d_{4}$ |$d_{5}$ |$d_{6}$ |$\\cdots$|$d_{d_{model}-1}$ |\n",
    "|----|----       |----       |----    |----   |----     |----    |----    |----    |----              |\n",
    "|I   | 0.1       | 0.4       | 0.9    |0.6    |$\\cdots$ |$\\cdots$|$\\cdots$|$\\cdots$|$\\cdots$          |\n",
    "|love| 0.4       | 0.8       | 0.2    |0.7    |$\\cdots$ |$\\cdots$|$\\cdots$|$\\cdots$|$\\cdots$          |\n",
    "|cats| 0.1       | 0.3       | 0.1    |0.2    |$\\cdots$ |$\\cdots$|$\\cdots$|$\\cdots$|$\\cdots$          |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int,sequence_length:int,p_drop:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout = nn.Dropout()\n",
    "        positional_embeddings = torch.zeros(size=(self.sequence_length,self.d_model),dtype=torch.float64)\n",
    "        for i in range(self.d_model):\n",
    "            for j in range(self.sequence_length):\n",
    "                if i%2==0:\n",
    "                    omega = torch.tensor(pow(1000,i/self.d_model))\n",
    "                    \n",
    "                    positional_embeddings[j][i] = sin(j/omega)\n",
    "                else:\n",
    "                    omega = torch.tensor(pow(1000,i-1/self.d_model))\n",
    "                    positional_embeddings[j][i] = torch.cos(j/omega)\n",
    "        self.positional_embeddings = positional_embeddings.unsqueeze(0)\n",
    "        self.register_buffer(name='positional_embeddings',tensor=self.positional_embeddings)\n",
    "        \n",
    "        \n",
    "    def forward(self,sequence:torch.Tensor):\n",
    "        if sequence.ndim==2:\n",
    "            sequence =sequence.unsqueeze(0)\n",
    "        assert sequence.shape[2] == self.d_model,f\"\"\"The embedding dimensions of model and input dont match model's dimensions : {self.d_model} , input dimensions : {sequence.shape[0]}\"\"\"\n",
    "        sequence =  sequence + (self.positional_embeddings[:,:sequence.shape[1],:]).requires_grad_(False)\n",
    "        return self.dropout(sequence)\n",
    "# pe = PositionalEncoding(d_model=4,sequence_length=3,p_drop=0.3) # 3 word sequence with each word represented as a vector of 4 numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network\n",
    "- In the paper each encoder decoder stack has it's own FFN layer. This is discussed in the section `3.3` of the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "- As per the paper, there are 3 steps which has 2 linear tranformations :\n",
    "\n",
    "    1. A linear layer with the output : $y_{1} = W_{1}(x) + b_{1}$\n",
    "    2. A relu operation with the output : $y_{relu} = max(0,y_{1})$\n",
    "    3. Another linear layer with the output : $y_{2} = W_{2}(y_{relu})+b_{2}$ \n",
    "- Final output : $y_{2} = W_{2}(max(0,W_{1}(x)+b_{1})) + b_{2}$\n",
    "- The first layer has input dimensions of $512(d_{model})$ and output dimension of $2048(d_{ffn})$\n",
    "- The second layer has input dimensions of $2048(d_{ffn})$ and output dimension of $512(d_{model})$\n",
    "- One question : What about the sequence ? How is the Sequence processed in the FFN ?\n",
    "- Simply one-by-one ( or we can say token-by-token )\n",
    "\n",
    "--- \n",
    "### Some detailed info about FFN Layer : \n",
    "- Input to `layer1` : $(1,1,d_{model})\\rightarrow \\textbf{ this is for a single word }$\n",
    "- This means that for the whole sequence which is `seq_len` long, Input : $(1,seq\\_len,d_{model})\\rightarrow\\textbf{this is for a single batch}$\n",
    "- This means for a whole batch which has `num_batch` entries : $(num\\_batch,seq\\_len,d_{model})$\n",
    "- Now comes the Output of `layer1` : \n",
    "    - since `layer1` has $d_{ff}$ number of neurons, therefore the output's shape will be : $(1,1,d_{ff})\\rightarrow \\textbf{ this is for a single word }$\n",
    "    - This means that for the whole sequence which is `seq_len` long, Input : $(1,seq\\_len,d_{ff})\\rightarrow\\textbf{this is for a single batch}$\n",
    "    - This means for a whole batch which has `num_batch` entries : $(num\\_batch,seq\\_len,d_{ff})$\n",
    "\n",
    "- `ReLu layer` doesn't modfiy the input shape of the input provided to it. So it passes it as it is. \n",
    "- \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_dim:int = 512, output_dim = 2048, p_dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer1 = nn.Linear(in_features=input_dim,out_features=output_dim)\n",
    "        self.linear_layer2 = nn.Linear(in_features=output_dim,out_features=input_dim)\n",
    "        self.dropout = nn.Dropout(p = p_dropout)\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return self.linear_layer2(self.dropout(self.linear_layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATTENTION\n",
    "- In the old sequence to sequence models, we had an encoder and a decoder. \n",
    "- We feed the Encoder with data in a sequential format and then feeding them to a decoder in the same sequential format. \n",
    "<div align=\"center\">\n",
    "<img src=\"encoder_decoder.gif\" width=\"700\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- After getting the whole input sequence, the encoder converts the whole sequence into a vector $\\overrightarrow{C}$ (that can be seen as the summary of the input text)\n",
    "\n",
    "- This vector $\\overrightarrow{C}$ is then passed into the decoder and the decoder converts this vector to probabilities of next possible tokens.\n",
    "\n",
    "#### ISSUE HERE : \n",
    "\n",
    "- For a long input sentence, it's not able to keep up with the context of the sentence and the vector $\\overrightarrow{C}$ that was supposed to carry the information to the decoder is overloaded with information. \n",
    "\n",
    "- Suppose we want to translate this sentence: बत्तियाँ बंद कर दो\n",
    "- The transaltion would be : Turn off the lights. \n",
    "<div align=\"center\">\n",
    "\n",
    "|Turn  | off| the | lights| \n",
    "|----- |----|----|----|\n",
    "|बत्तियाँ |बंद  |कर  |दो  |\n",
    "</div>\n",
    "\n",
    "- here to translate the first word `बत्तियाँ` we need to focus more on the word `lights` instead of the whole sentence. \n",
    "\n",
    "- similarly for the word `बंद` we need to focus more on the word `turn off`\n",
    "\n",
    "- This translation could be done properly by the LSTM if it could see the section of sentence where `बत्तियाँ` was used or where `बंद` was used instead of going through the entire sentence.\n",
    "\n",
    "- It would be preffered that the vector representation $\\overrightarrow{C}$ was more of dynamic than static. \n",
    "- We would want the LSTM to be able to specifically notice some part of the sentence for generating the probability of the most likely character for a particular state. \n",
    "- Or in other words we would want the LSTM to give more **Atttention** to a specific portion of sentence for generating the output for a particular state.\n",
    "\n",
    "----\n",
    "\n",
    "- Another point to note is that context in which the word was used is also important.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|`Apple` makes great mobile phones.|\n",
    "|-----|\n",
    "|An `apple` a day keeps the doctor away.|\n",
    "\n",
    "</div>\n",
    "\n",
    "- In 2 different sentences the meanings of the word `apple` changed. \n",
    "\n",
    "- Therefore the words surronding the word under consideration also affect the true meaning of the word under consideration. \n",
    "\n",
    "- Thereby the `attention` should not only focus the main words around which needed to be translated or interpreted but the actual meaning of the words should also be delivered by considering the surroundings. \n",
    "\n",
    "\n",
    "- **One question** : can't we just set out our word embeddings in such a way that we can capture the context of use of the word just by seeing it's word embeddings. \n",
    "\n",
    "- The answer is : \n",
    "     - Word embeddings do capture the meaning of the words but in a **average out format** i.e they try to capture all possible meanings of the word. \n",
    "\n",
    "     - Each dimension of the embeddings caputures a different meaning of a word hence the meaning of `apple` as a fruit and a brand is being captured in the word embeddings. It just so happens that we're looking for a more refined and specific meaning that word embeddings do have but in a **diluted** format. \n",
    "\n",
    "     - Word embeddings are created once but used countless times. **They are static**.\n",
    "     Hence re-training them is not a good idea!\n",
    "\n",
    "     - **ONE POINT TO NOTE** : This may also happen that the training data that was used to generate word embeddings may have the word `apple` used as a fruit more than as a brand. Therefore, there may be an **inherit bias** towards using the word `apple` as a friut than as a brand. To address this we need the help of surrounding words to evaluate the true usage of the word. \n",
    "\n",
    "For a better understanding of the expectation from the **attention** mechanism , consider the following example : \n",
    "<div align=\"center\">\n",
    "\n",
    "`Apple` launched a new mobile phone while I was eating an `apple`. \n",
    "\n",
    "</div>\n",
    "\n",
    "- Here in a single sentence the word `apple` was used in $2$ different meanings.\n",
    "\n",
    "- Here we want our **attention mechanism** to actually differentiate between the $2$ types of apples being talked about.\n",
    "\n",
    "- We want that based on the surrounding text of the $1^{st}$ `apple` , the weightage of the **technology/brand** aspect of the word should be given more weightage whereas for the the $2^{nd}$ `apple`, the **friut/edible** aspect of the word should be given more weightage.\n",
    "\n",
    "-----\n",
    "\n",
    "#### Self Attention \n",
    "- Suppose I have a sentence : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "`Apple` launched a new mobile phone. \n",
    "\n",
    "</div>\n",
    "\n",
    "- Here the word apple redirects us to the context of a brand. \n",
    "\n",
    "- Since this conclusion is based on the words surrounding the word `Apple`, therefore we must devise some mechanism to represent the word embeddings of the word `Apple` as some factor of the surrounding words.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "`Apple` $=\\alpha_{11}*(\\textbf{Apple}) +\\alpha_{12}*(\\textbf{lauched}) + \\alpha_{13}*(\\textbf{a})+\\alpha_{14}*(\\textbf{new})+\\alpha_{15}*(\\textbf{mobile})+\\alpha_{16}*(\\textbf{phone}) \\newline \\text{ where : } \\displaystyle\\sum_{i=0}^{n}\\alpha_{i}=1$ \n",
    "\n",
    "</div>\n",
    "\n",
    "- Similarly, for the word `launched`: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "`launched`$=\\alpha_{21}*(\\textbf{Apple}) +\\alpha_{22}*(\\textbf{lauched}) + \\alpha_{23}*(\\textbf{a})+\\alpha_{24}*(\\textbf{new})+\\alpha_{25}*(\\textbf{mobile})+\\alpha_{26}*(\\textbf{phone})$\n",
    "\n",
    "</div>\n",
    "\n",
    "and so on for other words. \n",
    "\n",
    "- **Note** that the words here are not literal words but rather word embeddings.  \n",
    "\n",
    "- Now comes the main question : what should be these $\\alpha_{ij}$'s should be?\n",
    "\n",
    "- One approach can be : \n",
    "     - Since we want to influence the word embeddings of the target word with the surrounding words that helps in defining the meaning of this, how about the similarity score between the 2 words ? ( basically the dot product )\n",
    "\n",
    "     - Suppose we want the coefficient $\\alpha_{15}$ i.e the simiarity between the words: `apple` and `phone` (took this deliberately to prove a point later.)\n",
    "\n",
    "     - Now if the words are of similar meaning then they have a high dot product. \n",
    "\n",
    "     - If the words or very unrelated then they will have a low dot product. \n",
    "     - Explanation : \n",
    "          - word embeddings are vectors that capute the meaning of a word in higher dimensional number spaces. \n",
    "          - words may have different meanings in different aspects. Since there can be various aspects in which a word can be used, therefore there are various dimensions where we can find out meaning captured in the vector. \n",
    "          <div align=\"center\">\n",
    "\n",
    "          $\\text{dot\\_product}(a,b)  = \\displaystyle\\sum_{i=0}^{d_{model}}a_{i}.b_{i}$\n",
    "\n",
    "          </div>\n",
    "          \n",
    "          - **Note** that  $\\text{dot\\_product}(a,b)$ is a scalar value and not a vector. \n",
    "\n",
    "          - Higher the value of $\\text{dot\\_product}(a,b)$ influence of $b$  will be more on the meaning of $a$ and will enhance the overall contextual meaing of $a$.\n",
    "\n",
    "          - Furthermore, other words in the sentence also contribute the new word embeddings of the current word by the same process.\n",
    "\n",
    "          - **NOTE**: we use normalized coefficients and not the exact value of dot_product operation. These normalized coefficients are produced by softmax function : \n",
    "          <div align=\"center\">\n",
    "\n",
    "          $\\alpha_{ij}  =  \\frac{\\displaystyle\\ e^{\\text{dot\\_product}(i,j)}}{\\displaystyle\\sum_{i=0}^{i=n}e^{\\text{dot\\_product}(i,j)}}$\n",
    "\n",
    "          </div>\n",
    "\n",
    "          - Benifits of this scheme :\n",
    "               - The process of calculation of refined-embeddings can be parallelized as there is no dependency of the next operation on the previous operation. But how? \n",
    "\n",
    "                    - Here we only need to compute the dot product of word embeddings of the words that we have. Here we don't need the inputs from the previous word or the sequence to compute the next set of word embeddings. \n",
    "\n",
    "                    - Suppose for the sequence : \n",
    "                      <div align=\"center\">\n",
    "\n",
    "                      `Apple launched new mobile`\n",
    "\n",
    "                      </div>\n",
    "\n",
    "                      - For this sentence, we can easily parallelize the following computations:\n",
    "\n",
    "                         - $\\text{dot\\_product ( Apple, launched ) }$\n",
    "\n",
    "                         - $\\text{dot\\_product ( Apple, new ) }$\n",
    "\n",
    "                         - $\\text{dot\\_product ( Apple, mobile )}$\n",
    "\n",
    "                         - $\\text{dot\\_product ( launched, new )}$\n",
    "\n",
    "                         - $\\text{dot\\_product ( new, mobile )}$\n",
    "                         \n",
    "               - There is no need of learning the parameters, a simple runtime computation can do the job as the word embeddings are readily available at hand. \n",
    "- Summarizing the whole process : \n",
    "     1) Input setence's word embeddings are fetched for each word. \n",
    "     2) We compute the dot product of each of the word embedding pair. \n",
    "     3) Before using them to find new word embedding for each sentence, take the softmax of dot product entity involved in making the vector. \n",
    "     4) Find the new word embeddings that will have the meanings caputred as per the surrounding words. \n",
    "\n",
    "\n",
    "---- \n",
    "#### Is everything alright ? \n",
    "\n",
    "- Below is the whole flow of the concept of attention discussed above : \n",
    "\n",
    "<img src=\"key_query_value.jpeg\">\n",
    "\n",
    "- One thing to note is that : The embeddings in **Green**, they are acting like someone whole is asking a questiong to all the words around it : Are you similar to me ? If yes then how much ? \n",
    "\n",
    "- Word embeddings used in this part of process of finding  self-attention is called **Query**. \n",
    "\n",
    "- The word embeddings in Yellow : they are the ones who're being asked the question and they're comparing their values with the values of the **Query** embedding.\n",
    "\n",
    "- These word embeddings which are used to compare the values of the Query embeddings are called **Keys**.\n",
    "\n",
    "- In the final stage, the Blue emebeddings are nothing but the emebeddings of the **Key**\n",
    "words that are now being multiplied by the coefficeints.\n",
    "\n",
    "- These embeddings are called **Value**\n",
    "\n",
    "- Symbols : \n",
    "     - **Key** : $K$ \n",
    "     - **Query** : $Q$ \n",
    "     - **Value** : $V$ \n",
    "\n",
    "- Hence we can say that word embeddings for each word can be used in $3$ different forms i.e as **Key**,**Query** $\\&$ **Value**.  \n",
    "\n",
    "- Now comes the main question : Do you think the **same word embedding** should be **used for all the 3 usage aspects**? \n",
    "\n",
    "- People say it's not. The reason for keeping the word emebddings different for the same word : \n",
    "     - # TODO : Get a convincing reason for it! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "- So now we know that we need different word embeddings to use as key , query and value vectors.\n",
    "\n",
    "- But how will we find them? \n",
    "\n",
    "- So now our question is : Given the word embeddings of the word W: \n",
    "     - What is the **transformation T** that's to be applied on W to get the **Key embeddings** or $K(W)$. \n",
    "          - $T_{key}(W) = K(W)$\n",
    "\n",
    "     - Similarly, for query and value : \n",
    "          - $T_{query}(W) = Q(W)$\n",
    "          - $T_{value}(W) = V(W)$\n",
    "     - Hence now our task is to evaluate the linear transformations: $T_{key}(W),T_{query}(W),T_{value}(W)$\n",
    "\n",
    "- Before jumping into how we're supposed to find, better look at exactly what are we gonna do: \n",
    "\n",
    "- We'll be having a vector for each word. \n",
    "- Now we will transfrom the vector into Key, Query and Value vector into some other vector by multiplying it from a Tranformation matrix. \n",
    "- Now how will we make this transformation matrix for each key query and value ? Simply by learning from data.\n",
    "\n",
    "- So essentially we have 3 matrices which we learn during the training process : $W^{k},W^{q}\\text{ \\& }W^{v} $ matrices. The embeddings are multiplied with these matrices to generate a new vector that can be used a key vector , query vector and a value vector. \n",
    "This is something we actually do in a linear transform as well! we use a matrix to modify a vector right. \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"key_query_value_matrix.jpeg\">\n",
    "</div>\n",
    "\n",
    "- The matrices shown above are learned from the data itself. \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views and contiguous in pytorch : [Reference Article](https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd)\n",
    "\n",
    "\n",
    "- The `.view` function in python does nothing much apart from returning the alternate way of **viewing** the data chunk\n",
    "\n",
    "- View is nothing but an alternative way to interpret the original tensor’s dimension without making a physical copy in the memory.\n",
    "\n",
    "- This means that any change in the view instance of the tensor will reflect in the orginial tensor as well since the view tensor is reading the data from the same memory address as of the original tensor.\n",
    "\n",
    "- Same goes for the fact when the orginal tensor is modified, the view tensor also gets changed for the very same reason. \n",
    "\n",
    "- Now certain operation in return a view and certain don't. \n",
    "- #TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important things to understand about pytorch dimensions : \n",
    "- We are gonna work with tensors that extend upto $3$ dimensions ( and $4$ dimensions as well 💀)\n",
    "- First get familiar with $dim = 3$ \n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain1.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain2.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain3.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    \n",
    "    - Say we want to get the sum. \n",
    "    - Now we can get it via 3 ways : \n",
    "- $dim = 0$\n",
    "    - Here the sum will be taken across the **batch** dimension like shown above. \n",
    "    - Basically, the pink tiles will be summed up and then same goes for the other cells. \n",
    "    - What's the output ? Say we had the input shape of $(3,4,5)$ \n",
    "    - We summed along the batch dimension for each element in the individual matrix. \n",
    "    - So final shape would be the shape of $(4,5)$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=0)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([4, 5])\n",
    ">>>tensor([[111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.]])\n",
    "```\n",
    "\n",
    "\n",
    "- $dim = 1$\n",
    "    - Here we sum acoss every column of the matrix . So essentially all the pink boxes are summed up for each matrix. \n",
    "    - Final Shape ? Here each matrix shrinks into a row where every element shows the sum of it's column and we have $3$ such matrices. So essentially 3 such rows and 5 columns. Thereby the final shape becomes $(3,5)$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=1)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    "\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([3, 5])\n",
    ">>>tensor([[   4.,    8.,   12.,   16.,   20.],\n",
    "           [  40.,   80.,  120.,  160.,  200.],\n",
    "           [ 400.,  800., 1200., 1600., 2000.]])\n",
    "```\n",
    "\n",
    "\n",
    "- $dim = 2$\n",
    "    - Here we're summing up the elements in a row. This means that all the green cells that we're able to see will shrink into one single cell for every matrix and we have $3$ such matrices. So this means that we'll be having $1$ column with sum of $4$ such rows and we have $3$ matrices . Therefore we convert the column into a row and stack $3$ such rows. Therefore the shape becomes $(3,4)$\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=2)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    "\n",
    "\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([3, 4])\n",
    ">>>tensor([[  15.,   15.,   15.,   15.],\n",
    ">>>        [ 150.,  150.,  150.,  150.],\n",
    ">>>        [1500., 1500., 1500., 1500.]])\n",
    "\n",
    "```\n",
    "\n",
    "- **NOTE** : final shape of matrix  : remove the dimension from the shape of the original matrix along which the operation was applied. Note that for $dim=3$, the structre is as follows : (Batch,Column,Row).\n",
    "\n",
    "- Similar logic goes for applying **softmax**. The dimension we specify in the softmax function, the softmax is applied along the elements of the same dimension.\n",
    "\n",
    "- $dim=0$ : Here the softmax would be applied for the **all the elements stackwise of every matrix**. \n",
    "\n",
    "- $dim=1$ : Here the softmax would be applied for the **elements column-wise for each column of every matrix**.\n",
    "\n",
    "- $dim=2$ : Here the softmax would be applied for the **elements row-wise for each row of every matrix**. \n",
    "\n",
    "- Note that softmax **doesn't produce a change in the final shape of the matrix**. It rather performs an **aggregation operation on a specified elements** in a dimension and **update those elements accordingly**.\n",
    "\n",
    "- Note that : \n",
    "<div align=\"center\">\n",
    "\n",
    "|dim:    |Batch|Column|Row |\n",
    "|--------|-----|------|----|\n",
    "|positive|$0$  |$1$   |$2$ |\n",
    "|negative|$-3$ |$-2$  |$-1$|\n",
    "</div>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last time : \n",
    "- We understood that there are $3$ spearate matrices that are used to generate the key, query and vlaue vectors for a single word.\n",
    "- But what are we actually finding out : \n",
    "\n",
    "$\\text{ATTENTION(K,Q,V) } =softmax(\\displaystyle\\frac{QK^{T}}{\\sqrt{d_{k}}})V $\n",
    "\n",
    "where : \n",
    "$K = \\text{ Key vector }$\n",
    "\n",
    "$Q = \\text{ Query vector }$\n",
    "\n",
    "$V = \\text{ Value vector }$\n",
    "\n",
    "$d_{k}= \\text{ dimension of key vectors }$\n",
    "\n",
    "\n",
    "- Here some question arise : what is the shape of the matrices that we're going to multiply our vector with ? \n",
    "- the answer is $d_{model} \\times d_{model}$\n",
    "- Here's what happens : \n",
    "- Suppose you have a sequence of words :\n",
    "<div align=\"center\">\n",
    "\n",
    "`My cat is a lovely cat.`😸\n",
    "\n",
    "</div>\n",
    "     \n",
    "    \n",
    "- each word has it's own word embedding : $e_{My}\\text{  }e_{cat}\\text{  }e_{is}\\text{  }e_{a}\\text{  }e_{lovely}\\text{  }e_{cat}$\n",
    "- Now merge these vectors into one single matrix i.e :\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|dimensions      |$d_{0}$|$d_{1}$|$d_{2}$|$d_{3}$|$\\cdots$|$d_{model}-1$|\n",
    "|----------------|-------|-------|-------|-------|--------|-------------|     \n",
    "|$e_{My}$        |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "|$e_{cat}$       |   102 |   452 |   12  |  212  |$\\cdots$|   864       |             \n",
    "|$e_{is}$        |   102 |   452 |   12  |  212  |$\\cdots$|   864       |     \n",
    "|$e_{a}$         |   102 |   452 |   12  |  212  |$\\cdots$|   864       | \n",
    "|$e_{lovely}$    |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "|$e_{cat}$       |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "\n",
    "</div>\n",
    "\n",
    "- So here the dimensions of the input are not just $1\\times d_{model}$ but actually $\\text{ sequence\\_length }\\times d_{model}$\n",
    "- Now we have 3 matrices each for $K,Q\\text{ \\& }V$\n",
    "- We multiply this matrix for each $K,Q,V$ :\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"attention_calculation.jpeg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- Here individual vectors show each key, query and value vector for each word.\n",
    "- Suppose you have a $5$ sequence length sentence as input, so to calcuate the contextual emebeddings of the $0^{th}$ word, we do the process decribed below: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"self_attention_calculation_process.jpeg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "- Here the scaled dot product is nothing but the division of $QK^{T}$ by $\\sqrt{d_{model}}$.\n",
    "\n",
    "- We'll explore why it's done later. **TODO**\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any Flaw yet ?\n",
    "- As of now we don't see any issue in out approach. \n",
    "- okay how about double meaning text 😏😼. Consider the text: \n",
    "<div align=\"center\">\n",
    "\n",
    "`A man saw a person with a telescope.`\n",
    "\n",
    "</div> \n",
    "\n",
    "- There 2 possible meaning are there : \n",
    "- The first person saw a man **using a telescope**. \n",
    "- The first person saw a man **holding a telescope**.\n",
    "- But in our current scenario, we only can capture only one type of meaning under current workflow. \n",
    "- So what's the fix ? \n",
    "- Instead of just one **type** of attention, calculate multiple attention scores. \n",
    "- Here comes the concept of **MULTIHEAD ATTENTION**. \n",
    "- Instead of calculating a single attention score, calculate multiple attention scores. \n",
    "- But how exactly will we do this ? \n",
    "- Above we saw that we're supposed to find the key, query, value matrix for fetching out the meaning of the current word given the words around it. \n",
    "- Here also we keep multiple key, query $\\&$ value matrices for capturing a different meaning but with a slightly different way : \n",
    "    - We break the vector alongside the dimensions . We know that the matrix we're working around it $sequence\\_length \\times d_{model}$. \n",
    "    - Suppose we want to capture 8 different meanings of the same word, so what we do is that we break the whole vector ( column wise ) into 8 different pieces :\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_splitting.jpeg\">\n",
    "\n",
    "\n",
    "    </div> \n",
    "\n",
    "- Here one important thing to note : $d_{model}\\text{  }\\% \\text{  }  \\text{number of attention heads }=0$\n",
    "- Why ? How the duck will you divide the $d_{model}$ into equal parts then!. \n",
    "- Now that we have broken down the matrix for multi-head attention, we'll do the same thing for each individual vector : find the contextual embeddings of each word and then concatenate the emebeddings.\n",
    "- How exactly ? Like this : \n",
    "\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_calculation_1.jpeg\">\n",
    "\n",
    "    </div> \n",
    "\n",
    "- Here we can see that we have calculated contextual embeddings for a single head. \n",
    "- A point to note that since each head has it's own set of key, query, and value pairs, there will be $\\text{num\\_heads}$ numbers of key, query, value matrices belonging to each head. \n",
    "\n",
    "- But this is for one head. We had $\\text{num\\_heads}$. Therefore we have $\\text{num\\_heads}$ numebr of contextual emebeddings. \n",
    "- Now what? We have $\\text{num\\_heads}$ number of matrices with the dimension $\\text{ sequence\\_length } \\times \\frac{d_{model}}{num\\_heads}$\n",
    "- We'll simply concatenate 😛:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"multihead_attention_calculation_2.jpeg\">\n",
    "</div> \n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME FINE DETAIL FOR GENERALITY : \n",
    "In the paper, attention is all you need, a more generalized approach towards calculating multihead attention was proposed : \n",
    "- We know that attention is : \n",
    "    - $\\text{ Attention}(K,Q,V)=\\textstyle{ softmax}(\\frac{\\textstyle{QK^{T}}}{\\textstyle\\sqrt{d_{k}}})V$\n",
    "    - Here the $d_{k}$ is the dimensions of the key matrix found by multiplying the word embeddings of input word by the key matrix for a given head. \n",
    "    - Note that the dimensions of $Q$ and $K$ must be $(sequence\\_length \\times d_{model})$ so that the product  $QK^{T}$ is valid. \n",
    "    - In the original paper :\n",
    "    - $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads})W^{0} $  \n",
    "    where $head_{i} = \\text{ Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ \n",
    "    where $W_{i}^{k,q,v}$  are the matrices that we use for converting simple word embeddings to key,query and value vectors. \n",
    "    - Wait a 🦆ing minute ! what's $W^{0}$, I'll come back to this later.\n",
    "    - Note that their dimensions are as follow : \n",
    "        - Q : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - K : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - V : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - $W_{i}^{Q} : \\textstyle{d_{model}} \\times \\textstyle{d_{key}}$\n",
    "        - $W_{i}^{K} : \\textstyle{d_{model}} \\times \\textstyle{d_{key}}$\n",
    "        - $W_{i}^{V} : \\textstyle{d_{model}} \\times \\textstyle{d_{value}}$\n",
    "    - wait a minute : How did this $d_{key} \\& d_{value}$ come into picture ? \n",
    "    - Just to add generality ! In the discussing above, we divided the matrix such that the the $d_{key} \\& d_{value}$ came out to be same. What if someone decided to take up different sizes for key-query and value matrices. \n",
    "    - Just in case, it's key-query and value, why not key , query and value ? beacuse they're supposed to be multiplied 😭 like this: \n",
    "    $QK^{T}$ so for them to multiply all along well, we'll need $d_{key}$ same in both key and query matrix. \n",
    "    Note that this difference in the dimensions of key query value is in the matrix that is used to generate the key query and value matrices.\n",
    "    - Note that we still want the $d_{key}$ to me a factor of $d_{model}$\n",
    "    - Now follow the flow : \n",
    "    $head_{i} = \\text{ Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) = \\textstyle{ softmax}(\\frac{\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}}}{\\textstyle\\sqrt{d_{k}}})VW_{i}^{V}$\n",
    "\n",
    "    $ QW_{i}^{Q} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{key}}) = (\\textstyle{sequence\\_length} \\times d_{key}) $\n",
    "\n",
    "    $ KW_{i}^{K} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{key}})  = (\\textstyle{sequence\\_length} \\times d_{key})$\n",
    "\n",
    "    $VW_{i}^{V} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{value}}) = (\\textstyle{sequence\\_length} \\times d_{value}) $\n",
    "\n",
    "\n",
    "    $\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}} : (\\textstyle{sequence\\_length} \\times \\textstyle{sequence\\_length})$\n",
    "\n",
    "    $\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}} :(\\textstyle{sequence\\_length} \\times {d_{value}}) $\n",
    "\n",
    "    - So the final shape of attention for a single head comes out to be : $(\\textstyle{sequence\\_length} \\times {d_{value}}) $\n",
    "\n",
    "    - Since $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads})$\n",
    "    $\\implies \\text{MULTIHEAD ATTENTION} = (\\textstyle{sequence\\_length} \\times (num\\_heads \\times d_{value})) $ \n",
    "    - Note that it's still a $2D$ Matrix, it's just that the columns of this matrix increased $num\\_heads$ times. \n",
    "    - Now here we have a smol 🤏 problem : \n",
    "        - the current shape is : $(\\textstyle{sequence\\_length} \\times (num\\_heads \\times d_{value}))$\n",
    "        - But for further steps we need the matrix in $(\\textstyle{sequence\\_length} \\times  d_{model})$\n",
    "        - How to convert ? 🤔💡\n",
    "        - Simple , multiply it by a matrix whose dimensions are $((num\\_heads \\times d_{value})\\times d_{model})$ which is nothing but $W^{0}$ as in the formula shown in the very beginning!\n",
    "        - That's why  $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads}) \\times \\textbf W^{0}$\n",
    "        - This $W^0$ is important in this aspect. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some noteworthy implementation details : \n",
    "\n",
    "- we're going not gonna make num_heads number of different matrices man 😩! \n",
    "\n",
    "- That's not handy to maintain and will need some different logic for training 💀.I know I can't even implement a simple backpropagation 😔, implementing for a custom system of which I've got no clue of isn't very convincing to go with 🤐.   \n",
    "- What we're gonaa do is rather something that we consider a smart move but it's really not cause we're not that smart as we think of ourselves 😔💡: \n",
    "     - Make a single matrix for Key query values for all the attention heads. \n",
    "     - Fragment the final matrix of key query values into the desired number of heads and then calculate the attention for each head.\n",
    "     - After you're done with calcuating the attention of each head, we'll simlpy \n",
    "     concatenate that as we planned before. \n",
    "     - Something like this : \n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_implementation_for_key_qeury_value_matrices.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    - Now one question : Wouldn't this kill  our very idea of multi-head attention 😨🥹?\n",
    "    - Isn't this simply calculating attention for once and not for the times the number of heads we have 😭? \n",
    "\n",
    "    - **Actually not!**\n",
    "    - We're seeing this in the wrong way!\n",
    "    - We're just postponing steps for our conveience. \n",
    "    - Instead of breaking the key query value matrix for each head, we're first calcuating the combined matrices for key query value and then break them down for calcuating attention.  \n",
    "```python\n",
    "import numpy as np \n",
    "word_vector = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "key = np.array([[1,2,3,4],\n",
    "                [1,2,3,4],\n",
    "                [1,2,3,4],\n",
    "                [1,2,3,4]\n",
    "            ])\n",
    "query = np.array([[1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4]\n",
    "            ])\n",
    "value = np.array([[1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4]\n",
    "            ])\n",
    "\n",
    "word_key = np.matmul(word_vector,key)\n",
    "word_query = np.matmul(word_vector,query)\n",
    "word_value = np.matmul(word_vector,value)\n",
    "\n",
    "word_key_part_1 = word_key[:,0:2]\n",
    "word_key_part_2 = word_key[:,2:4]\n",
    "\n",
    "word_query_part_1 = word_query[:,0:2]\n",
    "word_query_part_2 = word_query[:,2:4]\n",
    "\n",
    "word_value_part_1 = word_value[:,0:2]\n",
    "word_value_part_2 = word_value[:,2:4]\n",
    "\n",
    "\n",
    "attention_part1 = np.matmul(np.matmul(word_query_part_1,word_key_part_1.transpose())/np.sqrt(2),word_value_part_1)\n",
    "attention_part2 = np.matmul(np.matmul(word_query_part_2,word_key_part_2.transpose())/np.sqrt(2),word_value_part_2)\n",
    "\n",
    "key1 = np.array([[1,2,],\n",
    "                [1,2,],\n",
    "                [1,2,],\n",
    "                [1,2,]\n",
    "            ])\n",
    "query1 = np.array([[1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,]\n",
    "            ])\n",
    "value1 = np.array([[1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,]\n",
    "            ])\n",
    "\n",
    "key2 = np.array([[3,4,],\n",
    "                 [3,4,],\n",
    "                 [3,4,],\n",
    "                 [3,4,]\n",
    "            ])\n",
    "query2 = np.array([[3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,]\n",
    "            ])\n",
    "value2 = np.array([[3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,]\n",
    "            ])\n",
    "\n",
    "word_key1 = np.matmul(word_vector,key1)\n",
    "word_query1 = np.matmul(word_vector,query1)\n",
    "word_value1 = np.matmul(word_vector,value1)\n",
    "attention1 = np.matmul(np.matmul(word_query1,word_key1.transpose()/np.sqrt(2)),word_value1)\n",
    "\n",
    "\n",
    "word_key2 = np.matmul(word_vector,key2)\n",
    "word_query2 = np.matmul(word_vector,query2)\n",
    "word_value2 = np.matmul(word_vector,value2)\n",
    "attention2 = np.matmul(np.matmul(word_query2,word_key2.transpose()/np.sqrt(2)),word_value2)\n",
    "\n",
    "\n",
    "print(\"attention1 = \"attention1,)\n",
    "print(\"attention2 = \"attention2,)\n",
    "print(\"attention_part1 = \"attention_part1,)\n",
    "print(\"attention_part2 = \"attention_part2,)\n",
    "\n",
    "\n",
    "OUTPUTS : \n",
    "attention1 =      [[ 27435.74311004  54871.48622008]\n",
    "                   [ 71332.9320861  142665.8641722 ]]\n",
    "attention2 =      [[ 411536.14665057  548714.86220076]\n",
    "                   [1069993.98129148 1426658.64172198]]\n",
    "attention_part1 = [[ 27435.74311004  54871.48622008]\n",
    "                   [ 71332.9320861  142665.8641722 ]]\n",
    "attention_part2 = [[ 411536.14665057  548714.86220076]\n",
    "                   [1069993.98129148 1426658.64172198]]\n",
    "```\n",
    "- This is a small simulation of what we're doing here. \n",
    "- In the first case, we're calculating the attention without splitting the key query and value matrices. \n",
    "- In the second case we have splitted the key query value matrix and then we're calculating the attention. \n",
    "- In both the cases the correspoinding attention values came out to be same thus proving our point 😎. \n",
    "\n",
    "\n",
    "------\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A time for reality check ( yes, once again 😔)\n",
    "- Here above we said the we can use different dimensions for the key-query and value matrices but that's not quite true if we look it from the architectural point of view. \n",
    "- Here's why : \n",
    "    - Suppose we have $512$ dimensions for our word embeddings.\n",
    "    - Now we decide to take up $8$ attention heads. \n",
    "    - $\\implies d_{key} = 64$\n",
    "    - Now there are 8 key-value matrices. \n",
    "    - What about $d_{value}$?\n",
    "    - Consider that $d_{value} \\neq 64$\n",
    "    - This implies we'll have different number of value matrices which have some problems: \n",
    "        - How will we handle the $8$ resultant key query products ? \n",
    "        - With what value matrix we'll multiply the products since we don't have equal number of value matrices to multiply with the key-query product matrices. \n",
    "    - Hence for keeping our lives peaceful, we'll do $d_{key}=d_{value}=\\frac{d_{model}}{h}=\\frac{512}{8} = 64$  \n",
    "    - Now one important quesiton : What of the $W^{0}$ matrix ? Will we discard it off 😔? \n",
    "    - No, we'll keep it ( reason : TODO : FIND OUT ) \n",
    "\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOTHER CONCEPT : MASKING 🤡\n",
    "\n",
    "- While performing the operations on decoder, this is the general flow : \n",
    "- Suppose the sentence under consideration is : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**INPUT** \n",
    "|My | cat | is | a | beautiful | cat.| 😸 |\n",
    "|---|-----|----|---|-----------|-----|----|\n",
    "\n",
    "**OUTPUT**\n",
    "\n",
    "|मेरी | बिल्ली | एक | सुंदर | बिल्ली | है | 😸 |\n",
    "|----|------|----|------|------|---|-----|\n",
    "</div>\n",
    "\n",
    "- In the encoder it will be converted to a matrix of embeddings and positional embeddings. \n",
    "- The input will essentially look like this before entering the decoder : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    " (say $d_{model}=5$ )\n",
    "| My      |  $1$  |  $3$  |  $4$ |  $8$ |  $2$ |\n",
    "|---------|-------|-------|------|------|------|\n",
    "| cat     |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ | \n",
    "| is      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  a      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|beautiful|  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  cat    |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "- Now there are 2 scenarios which can be considered : \n",
    "     - Training \n",
    "     - Inference ( basically running the trained model for use )\n",
    "\n",
    "- Case 1 : **INFERENCE MODE**\n",
    "     - Here since the traditional method is to pass on the inputs of the previous decoder unit, there is no scope of parallelism. \n",
    "     - Something like this happens : \n",
    "     \n",
    "     <div align=\"center\">\n",
    "\n",
    "     <img src=\"inference_in_transformer.jpg\">\n",
    "\n",
    "     </div>\n",
    "\n",
    "     - So if we have say $100$ words, this has to take $100$ units of time given $1$ computation takes $1s$. \n",
    "\n",
    "\n",
    "- Case 2 : **TRANING MODE** : \n",
    "     - Here the story is a bit different. \n",
    "     - Here something similar happens but with a twist : \n",
    "          - Take the same translation example as above. \n",
    "          - Suppose instead of producing the first token as **मेरी**, the token produced was **हमारी**. \n",
    "          <div align=\"center\">\n",
    "\n",
    "          <img src=\"traninig_mode_in_transformer.jpg\"> \n",
    "\n",
    "          </div>\n",
    "\n",
    "          - Now here instead of passing **हमारी** in the next component of the decoder we pass on the True value that should have come at that place to train the model with fact that we want **बिल्ली** after the token **मेरी**. \n",
    "          - So the question arises : Do we need to actually know the **pervious state through the transformer** ? Or we simply know what's the next state gonna be ?\n",
    "          Also if know the next state , why to even perform the calculation using the transformer ? \n",
    "          - So the answer to the last question is **TRAINING🤡**. How the hell are we supposed to calculate the loss without knowing what the transformer knows and what we want it to know and make it learn. \n",
    "          - The answer to the second question is : Yes we do! from the training data itself. \n",
    "          - The answer is the first question is : No , we simply don't. Our sole purpose of using performing this operation is to train the transformer.\n",
    "          - Since we know what's gonna be the next state , can we parallelize this operation for a single sequence ? The answer is Yes we can and that's what we do. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SO EVERYTHING LOOKS FINE TILL NOW ? \n",
    "- One wise man said, if you think it going to end well, then you're not paying attention 💀.\n",
    "- Here there's one issue while training : \n",
    "- When we send the embeddings into the decoder parallely, what essentially we're doing is: \n",
    "      - Suppose we send the sentence : \n",
    "<div align=\"center\">\n",
    "\n",
    "|My | cat | is | a | beautiful | cat.| 😸 |\n",
    "|---|-----|----|---|-----------|-----|---- |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- here when we sent the word **My**, we're essentially sending the information of what's going to come next in our sentence . \n",
    "- How so ? \n",
    "- Like this : \n",
    "- We wanted sent this input to the decoder : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "(say $d_{model}=5$ )\n",
    "\n",
    "| My      |  $1$  |  $3$  |  $4$ |  $8$ |  $2$ |\n",
    "|---------|-------|-------|------|------|------|\n",
    "| cat     |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ | \n",
    "| is      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  a      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|beautiful|  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  cat    |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "           \n",
    "</div>\n",
    "\n",
    "- We sent the contextual embeddings ! \n",
    "- These emebeddings contain the information about the words that came before and **AFTER** the current word. \n",
    "- But how ? \n",
    "- Like this : \n",
    "- What we learned before was that we need to express the embedding of the current word in terms of it's own meaning and the words around it. \n",
    "<div align=\"center\">\n",
    "\n",
    "Say we're considering the word **beautiful**:\n",
    "\n",
    "$e_{beautiful} = \\alpha_{1}.e_{my} + \\alpha_{2}.e_{cat} + \\alpha_{3}.e_{is} + \\alpha_{4}.e_{a} + \\alpha_{5}.e_{beautiful} + \\alpha_{6}.e_{cat} $\n",
    "</div>\n",
    "\n",
    "- Till $\\alpha_{5}.e_{beautiful}$ was okay because till that point we had the context of the words that came before. But the moment the last embedding vector term came into picture  $ \\alpha_{6}.e_{cat} $ , it kind of gave off the hint to the transformer that the next word is cat which shouldn't happen. \n",
    "- Ideally we would want to **HIDE** this information that the word **cat** is going to come up because in that case it'll fail to learn to learn. \n",
    "- So what we want is that while processing the inputs, we keep the coefficients of the words that are comming after the current word to be zero. \n",
    "- That means : \n",
    "      - If our current word is Cat ( the first occurance ) then the values : \n",
    "      $\\alpha_{3} = \\alpha_{4} =  \\alpha_{5} =  \\alpha_{6} = 0$\n",
    "- So ideally speaking, this should be done at the step of self attention calculation.\n",
    "- There we calculated the coefficient matrix for multiplying it with the word embeddings. \n",
    "- $softmax(\\displaystyle\\frac{QK^{T}}{\\sqrt{d_{k}}})$ this step to be specific. \n",
    "- The output of this operation is actually the coefficients for the value matrix. \n",
    "- For better understanding, see this : \n",
    "<div align=\"center\">\n",
    "\n",
    "|          |My        |cat       |is        |a         |beautiful|cat       |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|My        |  1       |   5      |   6      |    6     |   3     |   5      |              \n",
    "|cat       |  2       |   5      |   6      |    6     |   3     |   5      |              \n",
    "|is        |  3       |   5      |   6      |    6     |   3     |   5      |    \n",
    "|a         |  4       |   5      |   6      |    6     |   3     |   5      |    \n",
    "|beautiful |  5       |   5      |   6      |    6     |   3     |   5      |         \n",
    "|cat       |  5       |   5      |   6      |    6     |   3     |   5      |              \n",
    "\n",
    "</div>\n",
    "\n",
    "- Here for the first row, the values in the row represent the coefficeints that are to be multiplied with the value vectors of the corresponding words . \n",
    "- Now here what we would like to have is something like this : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|          |My        |cat       |is        |a         |beautiful|cat       |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|My        |  1       |   0      |   0      |    0     |   0     |   0      |              \n",
    "|cat       |  2       |   5      |   0      |    0     |   0     |   0      |              \n",
    "|is        |  3       |   5      |   6      |    0     |   0     |   0      |    \n",
    "|a         |  4       |   5      |   6      |    6     |   0     |   0      |    \n",
    "|beautiful |  5       |   5      |   6      |    6     |   3     |   0      |         \n",
    "|cat       |  5       |   5      |   6      |    6     |   3     |   5      |              \n",
    "\n",
    "</div>\n",
    "- So to achieve this what we do is : \n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"masking_demo.jpg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "- Take a mask matrix and add it to the scaled matrix before performing softmax. \n",
    "- $softmax(-\\infty)=0$\n",
    "- This makes the coefficient of the future terms to be $0$\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection : TODO😭\n",
    "- They are specifically placed at many places. \n",
    "- Study [this paper](https://arxiv.org/pdf/1603.05027) for better understanding of why residual connections workout well for large and deep models!\n",
    "- This one got a lot of math to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model:int = 512,h:int = 8, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # expected input dimesions in each word\n",
    "        \n",
    "        self.h  = h   # number of heads \n",
    "        assert d_model%h == 0 ,\"Expected values of d_model and h such that d_model is divisible by h\"\n",
    "        self.d_key = d_model//h\n",
    "        self.w_query = nn.Linear(d_model,d_model) # weight matrix for query \n",
    "        self.w_key = nn.Linear(d_model,d_model) # weight matrix for key \n",
    "        \n",
    "        \n",
    "        # Note that here we're  going for d_model X d_model and not for d_model X (h*d_value)\n",
    "        # Since we're given an input matrix of d_model.  \n",
    "        self.w_value = nn.Linear(d_model,d_model) # weight matrix for value \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "    \n",
    "    \n",
    "    @staticmethod # this is a static method and can be accessed outside the class using this: MultiHeadAttention.attention(input parameters)\n",
    "    def attention(self,key:torch.Tensor,query:torch.Tensor,value:torch.Tensor,mask):\n",
    "        key_query = torch.matmul(query,key.T)\n",
    "        scaled = key_query/torch.sqrt(self.d_key)  \n",
    "        # At this point , the shape of the matrix is : (Batch , h , seqeuence_length , seqeuence_length)\n",
    "        if mask is not None:\n",
    "            scaled.masked_fill(mask==0,-1e9)\n",
    "            \n",
    "        softmax_ = scaled.softmax(dim=-1)\n",
    "        \n",
    "        return torch.matmul(softmax_,value)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self,query,key,value,mask):\n",
    "        q_w = self.w_query(query) # query matrix for input sequence , Shape : (Batch , sequence_length , d_model) \n",
    "        k_w = self.w_key(key)     # key matrix for input sequence   , Shape : (Batch , sequence_length , d_model)\n",
    "        v_w = self.w_value(value) # value matrix for input sequence , Shape : (Batch , sequence_length , d_model)\n",
    "        batch,sequence_length,d_model = q_w.shape\n",
    "        \n",
    "        # This is done to break the resultant query matrix of the sequence into h matrices\n",
    "        # transpose is mainly done to make the matrix head-based indexable. \n",
    "        q_q_head_wise = q_w.view(batch,sequence_length,self.h,self.d_key).transpose(2,1) \n",
    "        q_k_head_wise = q_w.view(batch,sequence_length,self.h,self.d_key).transpose(2,1) \n",
    "        q_v_head_wise = q_w.view(batch,sequence_length,self.h,self.d_key).transpose(2,1) \n",
    "        attention_score = MultiHeadAttention.attention(self,q_k_head_wise,q_q_head_wise,q_v_head_wise,mask)\n",
    "        \n",
    "        #TODO : Study contiguous memory allocation in detail. \n",
    "        \n",
    "        attention_score = attention_score.transpose(1,2).contiguous().view(batch,sequence_length,self.h*self.d_key)\n",
    "        # Tanspose to get the dimension normally indexable and merging the tensors back. \n",
    "        \n",
    "        return self.w_o(attention_score)\n",
    "    \n",
    "    \n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,epsillon: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.epsillon = epsillon\n",
    "        self.aplha = nn.Parameter(torch.ones(size=1))\n",
    "        self.beta  = nn.Parameter(torch.ones(size=1))\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean = x.mean(dim = -1,keepdim=True)\n",
    "        std  = x.std(dim = -1,keepdim=True) \n",
    "        normalized = self.aplha * (x-mean)/(std + self.epsillon) + self.beta\n",
    "        return normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalize = LayerNormalization(epsillon=1e-8)\n",
    "    def forward(self,x:torch.Tensor,sub_layer):\n",
    "        # here the logic of sublayer is that the operations are performed on the input using some layer be if feed-forward or muti-head attention these 2 shall act as sublayers and then provide the output for it. Once we have the output we can continue with the addition of output and the input and then normalize them as the name says \"add and norm\" in the paper.  \n",
    "        return self.normalize(x + sub_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model : int ,\n",
    "                 h : int , \n",
    "                 dropout : float,\n",
    "                 feed_forward_output : int , \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model,h=h,dropout=dropout)\n",
    "        self.feed_forward_block = FeedForwardNeuralNetwork(input_dim=d_model,\n",
    "                                                           output_dim = feed_forward_output,\n",
    "                                                           p_dropout=dropout)\n",
    "        # this is done to actually provide more flexibility for using skip connections properly. Since these skip connections are to be used 2 times here that too in between of serveral other connections. So we're essentially using it for skipping out on extra components and just focus on the residual connection.  \n",
    "        \n",
    "        self.skip_connections  = nn.ModuleList([ResidualConnection() for _ in range(2)])\n",
    "        \n",
    "    def forward(self,x:torch.Tensor,mask:torch.Tensor):\n",
    "        # first we pass the input to the multi-head attention layer. \n",
    "        x1 = self.skip_connections[0](x,lambda x: self.self_attention(x,x,x,mask))\n",
    "        x2 = self.skip_connections[1](x1,lambda x1:self.feed_forward_block(x1))\n",
    "        return x2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_stacks: int = 6,\n",
    "                 d_model : int = 512 , \n",
    "                 h : int = 8 , \n",
    "                 dropout : float = 0.1 , \n",
    "                 feed_forward_output : int = 2048 \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.normalize = LayerNormalization()\n",
    "        \n",
    "        self.encoder_list = nn.ModuleList([EncoderBlock(d_model=d_model,\n",
    "                                                        h = h,\n",
    "                                                        dropout=dropout,\n",
    "                                                        feed_forward_output=feed_forward_output)\n",
    "                                           for _ in range(num_stacks)])\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        for layers in self.encoder_list:\n",
    "            x = layers(x,mask)\n",
    "        return self.normalize(x) # TODO : Check out in which part the normalization at this step is prescribed. Add that into the notes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model : int ,\n",
    "                 h : int , \n",
    "                 dropout : float,\n",
    "                 feed_forward_output : int , \n",
    "                ) -> None:\n",
    "        super().__init__(self )\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "                                                 d_model=d_model,\n",
    "                                                 h = h,\n",
    "                                                 dropout=dropout\n",
    "                                                )\n",
    "        \n",
    "        self.cross_attention = MultiHeadAttention(\n",
    "                                                 d_model=d_model,\n",
    "                                                 h = h,\n",
    "                                                 dropout=dropout\n",
    "                                                 )\n",
    "        \n",
    "        self.feed_forward_layers = FeedForwardNeuralNetwork(\n",
    "                                                            input_dim=d_model,\n",
    "                                                            output_dim=feed_forward_output,\n",
    "                                                            p_dropout=dropout\n",
    "                                                           )\n",
    "        \n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection() for _ in range(3)])\n",
    "        \n",
    "    def forward(self,x:torch.Tensor,\n",
    "                encoder_outputs : torch.Tensor,\n",
    "                source_mask:torch.Tensor,\n",
    "                target_mask:torch.Tensor,\n",
    "                ):\n",
    "        # TODO : understand the working of this source_mask and target_mask . this is something that I didn't find in the paper nor the tutorial explained quite clear\n",
    "        x1 = self.residual_connection[0](x , lambda x: self.self_attention(x,x,x,target_mask))\n",
    "        # TODO : Check this one out : What I've understood from the varius sources is that the encoder output is used as the query and key values and the encoded values produced by the decoder's self_attention are used as the value vector. In the function implementaiton of multihead attention, we've used the sequecne of query key value as input, but in the tutorial they've use the  same sequnce but while calling it here in the second part where we needed the values from the decoder as well in the form of value vector. In the tutorial they passed it as x1,encoder_output , encoder_output which is kind of wrong. \n",
    "        \n",
    "        \n",
    "        x2 = self.residual_connection[1](x1, lambda x1:self.cross_attention(encoder_outputs,encoder_outputs,x1,source_mask))\n",
    "        x3 = self.residual_connection[2](x2,lambda x2 : self.feed_forward_layers(x2))\n",
    "        return x3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_stack:int = 6,\n",
    "                 d_model : int = 512,\n",
    "                 h : int = 8, \n",
    "                 dropout : float = 0.1,\n",
    "                 feed_forward_output : int = 2048 ,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.normalize = LayerNormalization()\n",
    "        \n",
    "        self.decoder_list = nn.ModuleList([DecoderBlock(\n",
    "                                                        d_model=d_model,\n",
    "                                                        h = h,\n",
    "                                                        dropout=dropout,\n",
    "                                                        feed_forward_output=feed_forward_output\n",
    "                                                      ) for _ in range(num_stack)])\n",
    "    \n",
    "    def forward(self,x,encoder_output,source_mask,target_mask):\n",
    "        for layer in self.decoder_list:\n",
    "            x = layer(x,encoder_output,source_mask,target_mask)\n",
    "        return self.normalize(x) # TODO : Check out in which part the normalization at this step is prescribed. Add that into the notes.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, vocab_size : int , d_model : int = 512) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,out_features=vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return torch.log_softmax(self.linear_layer(x),dim = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder:Encoder,\n",
    "                 decoder:Decoder,\n",
    "                 source_embedding: InputEmbeddings,\n",
    "                 target_embedding: InputEmbeddings,\n",
    "                 source_positional_encoding:PositionalEncoding,\n",
    "                 target_positional_encoding:PositionalEncoding,\n",
    "                 projection_layer:ProjectionLayer\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embedding = source_embedding\n",
    "        self.target_embedding = target_embedding\n",
    "        self.source_positional_encoding = source_positional_encoding\n",
    "        self.target_positional_encoding = target_positional_encoding\n",
    "        self.projection_layer = projection_layer\n",
    "    def encode(self,\n",
    "               source,\n",
    "               source_mask\n",
    "              ):\n",
    "        input_embeddings = self.source_embedding(source)\n",
    "        input_embeddings_with_positional_encodings = self.source_positional_encoding(input_embeddings)\n",
    "        encodings = self.encoder(input_embeddings_with_positional_encodings,\n",
    "                                 source_mask)\n",
    "        return encodings\n",
    "    \n",
    "    \n",
    "    def decode(self,encoder_output,source_mask,target,target_mask):\n",
    "        target_embeddings = self.target_embedding(target)\n",
    "        target_embeddings_with_positional_encodings = self.target_positional_encoding(target_embeddings)\n",
    "        decodings = self.decoder(target_embeddings_with_positional_encodings,\n",
    "                                 encoder_output,\n",
    "                                 source_mask,\n",
    "                                 target_mask)\n",
    "        return decodings\n",
    "    \n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_vocab_size:int,\n",
    "                      target_vocab_size:int,\n",
    "                      max_input_seq_lenght : int ,\n",
    "                      max_target_seq_lenght : int ,\n",
    "                      d_model : int  = 512,\n",
    "                      dropout : float = 0.1, \n",
    "                      num_stacks : int = 6, \n",
    "                      num_attention_heads : int = 8, \n",
    "                      feed_forward_num_out : int = 2048)->Transformer:\n",
    "    encoder= Encoder(\n",
    "                      num_stacks=num_stacks,\n",
    "                      d_model=d_model,\n",
    "                      h = num_attention_heads,\n",
    "                      dropout=dropout,\n",
    "                      feed_forward_output=feed_forward_num_out)\n",
    "    decoder= Decoder(num_stack=num_stacks,\n",
    "                     d_model=d_model,\n",
    "                     h=num_attention_heads,\n",
    "                     dropout=dropout,\n",
    "                     feed_forward_output=feed_forward_num_out)\n",
    "    source_embedding=  InputEmbeddings(\n",
    "                                        d_model=d_model,\n",
    "                                        vocab_size=input_vocab_size\n",
    "                                      )\n",
    "    target_embedding=  InputEmbeddings(\n",
    "                                        d_model=d_model,\n",
    "                                        vocab_size=target_vocab_size\n",
    "                                      )\n",
    "    source_positional_encoding = PositionalEncoding(\n",
    "                                                     d_model=d_model,\n",
    "                                                     sequence_length=max_input_seq_lenght,\n",
    "                                                     p_drop=dropout\n",
    "                                                   )\n",
    "    target_positional_encoding= PositionalEncoding(\n",
    "                                                    d_model=d_model,\n",
    "                                                    sequence_length=max_target_seq_lenght\n",
    "                                                  )\n",
    "    projection_layer= ProjectionLayer(  \n",
    "                                        vocab_size=target_vocab_size,\n",
    "                                        d_model=d_model\n",
    "                                     )\n",
    "    \n",
    "    transformer_object = Transformer(\n",
    "                                      encoder=encoder,\n",
    "                                      decoder=decoder,\n",
    "                                      source_embedding=source_embedding,\n",
    "                                      target_embedding=target_embedding,\n",
    "                                      source_positional_encoding=source_positional_encoding,\n",
    "                                      target_positional_encoding=target_positional_encoding,\n",
    "                                      projection_layer=projection_layer)\n",
    "    \n",
    "    \n",
    "    # for p in transformer_object.parameters():\n",
    "    #   print(p)\n",
    "    return transformer_object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
