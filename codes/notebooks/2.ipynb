{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important things to understand about pytorch dimensions : \n",
    "- We are gonna work with tensors that extend upto $3$ dimensions ( and $4$ dimensions as well 💀)\n",
    "- First get familiar with $dim = 3$ \n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain1.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain2.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"dim_explain3.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    \n",
    "    - Say we want to get the sum. \n",
    "    - Now we can get it via 3 ways : \n",
    "- $dim = 0$\n",
    "    - Here the sum will be taken across the **batch** dimension like shown above. \n",
    "    - Basically, the pink tiles will be summed up and then same goes for the other cells. \n",
    "    - What's the output ? Say we had the input shape of $(3,4,5)$ \n",
    "    - We summed along the batch dimension for each element in the individual matrix. \n",
    "    - So final shape would be the shape of $(4,5)$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=0)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([4, 5])\n",
    ">>>tensor([[111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.],\n",
    "           [111., 222., 333., 444., 555.]])\n",
    "```\n",
    "\n",
    "\n",
    "- $dim = 1$\n",
    "    - Here we sum acoss every column of the matrix . So essentially all the pink boxes are summed up for each matrix. \n",
    "    - Final Shape ? Here each matrix shrinks into a row where every element shows the sum of it's column and we have $3$ such matrices. So essentially 3 such rows and 5 columns. Thereby the final shape becomes $(3,5)$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=1)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    "\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([3, 5])\n",
    ">>>tensor([[   4.,    8.,   12.,   16.,   20.],\n",
    "           [  40.,   80.,  120.,  160.,  200.],\n",
    "           [ 400.,  800., 1200., 1600., 2000.]])\n",
    "```\n",
    "\n",
    "\n",
    "- $dim = 2$\n",
    "    - Here we're summing up the elements in a row. This means that all the green cells that we're able to see will shrink into one single cell for every matrix and we have $3$ such matrices. So this means that we'll be having $1$ column with sum of $4$ such rows and we have $3$ matrices . Therefore we convert the column into a row and stack $3$ such rows. Therefore the shape becomes $(3,4)$\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "print(\"x.shape = \",x.shape)\n",
    "sm = x.sum(dim=2)\n",
    "print(\"sm.shape = \",sm.shape)\n",
    "print(sm)\n",
    "\n",
    "\n",
    ">>>x.shape =  torch.Size([3, 4, 5])\n",
    ">>>sm.shape =  torch.Size([3, 4])\n",
    ">>>tensor([[  15.,   15.,   15.,   15.],\n",
    ">>>        [ 150.,  150.,  150.,  150.],\n",
    ">>>        [1500., 1500., 1500., 1500.]])\n",
    "\n",
    "```\n",
    "\n",
    "- **NOTE** : final shape of matrix  : remove the dimension from the shape of the original matrix along which the operation was applied. Note that for $dim=3$, the structre is as follows : (Batch,Column,Row).\n",
    "\n",
    "- Similar logic goes for applying **softmax**. The dimension we specify in the softmax function, the softmax is applied along the elements of the same dimension.\n",
    "\n",
    "- $dim=0$ : Here the softmax would be applied for the **all the elements stackwise of every matrix**. \n",
    "\n",
    "- $dim=1$ : Here the softmax would be applied for the **elements column-wise for each column of every matrix**.\n",
    "\n",
    "- $dim=2$ : Here the softmax would be applied for the **elements row-wise for each row of every matrix**. \n",
    "\n",
    "- Note that softmax **doesn't produce a change in the final shape of the matrix**. It rather performs an **aggregation operation on a specified elements** in a dimension and **update those elements accordingly**.\n",
    "\n",
    "- Note that : \n",
    "<div align=\"center\">\n",
    "\n",
    "|dim:    |Batch|Column|Row |\n",
    "|--------|-----|------|----|\n",
    "|positive|$0$  |$1$   |$2$ |\n",
    "|negative|$-3$ |$-2$  |$-1$|\n",
    "</div>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last time : \n",
    "- We understood that there are $3$ spearate matrices that are used to generate the key, query and vlaue vectors for a single word.\n",
    "- But what are we actually finding out : \n",
    "\n",
    "$\\text{ATTENTION(K,Q,V) } =softmax(\\displaystyle\\frac{QK^{T}}{\\sqrt{d_{k}}})V $\n",
    "\n",
    "where : \n",
    "$K = \\text{ Key vector }$\n",
    "\n",
    "$Q = \\text{ Query vector }$\n",
    "\n",
    "$V = \\text{ Value vector }$\n",
    "\n",
    "$d_{k}= \\text{ dimension of key vectors }$\n",
    "\n",
    "\n",
    "- Here some question arise : what is the shape of the matrices that we're going to multiply our vector with ? \n",
    "- the answer is $d_{model} \\times d_{model}$\n",
    "- Here's what happens : \n",
    "- Suppose you have a sequence of words :\n",
    "<div align=\"center\">\n",
    "\n",
    "`My cat is a lovely cat.`😸\n",
    "\n",
    "</div>\n",
    "     \n",
    "    \n",
    "- each word has it's own word embedding : $e_{My}\\text{  }e_{cat}\\text{  }e_{is}\\text{  }e_{a}\\text{  }e_{lovely}\\text{  }e_{cat}$\n",
    "- Now merge these vectors into one single matrix i.e :\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|dimensions      |$d_{0}$|$d_{1}$|$d_{2}$|$d_{3}$|$\\cdots$|$d_{model}-1$|\n",
    "|----------------|-------|-------|-------|-------|--------|-------------|     \n",
    "|$e_{My}$        |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "|$e_{cat}$       |   102 |   452 |   12  |  212  |$\\cdots$|   864       |             \n",
    "|$e_{is}$        |   102 |   452 |   12  |  212  |$\\cdots$|   864       |     \n",
    "|$e_{a}$         |   102 |   452 |   12  |  212  |$\\cdots$|   864       | \n",
    "|$e_{lovely}$    |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "|$e_{cat}$       |   102 |   452 |   12  |  212  |$\\cdots$|   864       |         \n",
    "\n",
    "</div>\n",
    "\n",
    "- So here the dimensions of the input are not just $1\\times d_{model}$ but actually $\\text{ sequence\\_length }\\times d_{model}$\n",
    "- Now we have 3 matrices each for $K,Q\\text{ \\& }V$\n",
    "- We multiply this matrix for each $K,Q,V$ :\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"attention_calculation.jpeg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- Here individual vectors show each key, query and value vector for each word.\n",
    "- Suppose you have a $5$ sequence length sentence as input, so to calcuate the contextual emebeddings of the $0^{th}$ word, we do the process decribed below: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"self_attention_calculation_process.jpeg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "- Here the scaled dot product is nothing but the division of $QK^{T}$ by $\\sqrt{d_{model}}$.\n",
    "\n",
    "- We'll explore why it's done later. **TODO**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any Flaw yet ?\n",
    "- As of now we don't see any issue in out approach. \n",
    "- okay how about double meaning text 😏😼. Consider the text: \n",
    "<div align=\"center\">\n",
    "\n",
    "`A man saw a person with a telescope.`\n",
    "\n",
    "</div> \n",
    "\n",
    "- There 2 possible meaning are there : \n",
    "- The first person saw a man **using a telescope**. \n",
    "- The first person saw a man **holding a telescope**.\n",
    "- But in our current scenario, we only can capture only one type of meaning under current workflow. \n",
    "- So what's the fix ? \n",
    "- Instead of just one **type** of attention, calculate multiple attention scores. \n",
    "- Here comes the concept of **MULTIHEAD ATTENTION**. \n",
    "- Instead of calculating a single attention score, calculate multiple attention scores. \n",
    "- But how exactly will we do this ? \n",
    "- Above we saw that we're supposed to find the key, query, value matrix for fetching out the meaning of the current word given the words around it. \n",
    "- Here also we keep multiple key, query $\\&$ value matrices for capturing a different meaning but with a slightly different way : \n",
    "    - We break the vector alongside the dimensions . We know that the matrix we're working around it $sequence\\_length \\times d_{model}$. \n",
    "    - Suppose we want to capture 8 different meanings of the same word, so what we do is that we break the whole vector ( column wise ) into 8 different pieces :\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_splitting.jpeg\">\n",
    "\n",
    "\n",
    "    </div> \n",
    "\n",
    "- Here one important thing to note : $d_{model}\\text{  }\\% \\text{  }  \\text{number of attention heads }=0$\n",
    "- Why ? How the duck will you divide the $d_{model}$ into equal parts then!. \n",
    "- Now that we have broken down the matrix for multi-head attention, we'll do the same thing for each individual vector : find the contextual embeddings of each word and then concatenate the emebeddings.\n",
    "- How exactly ? Like this : \n",
    "\n",
    "\n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_calculation_1.jpeg\">\n",
    "\n",
    "    </div> \n",
    "\n",
    "- Here we can see that we have calculated contextual embeddings for a single head. \n",
    "- A point to note that since each head has it's own set of key, query, and value pairs, there will be $\\text{num\\_heads}$ numbers of key, query, value matrices belonging to each head. \n",
    "\n",
    "- But this is for one head. We had $\\text{num\\_heads}$. Therefore we have $\\text{num\\_heads}$ numebr of contextual emebeddings. \n",
    "- Now what? We have $\\text{num\\_heads}$ number of matrices with the dimension $\\text{ sequence\\_length } \\times \\frac{d_{model}}{num\\_heads}$\n",
    "- We'll simply concatenate 😛:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"multihead_attention_calculation_2.jpeg\">\n",
    "</div> \n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME FINE DETAIL FOR GENERALITY : \n",
    "In the paper, attention is all you need, a more generalized approach towards calculating multihead attention was proposed : \n",
    "- We know that attention is : \n",
    "    - $\\text{ Attention}(K,Q,V)=\\textstyle{ softmax}(\\frac{\\textstyle{QK^{T}}}{\\textstyle\\sqrt{d_{k}}})V$\n",
    "    - Here the $d_{k}$ is the dimensions of the key matrix found by multiplying the word embeddings of input word by the key matrix for a given head. \n",
    "    - Note that the dimensions of $Q$ and $K$ must be $(sequence\\_length \\times d_{model})$ so that the product  $QK^{T}$ is valid. \n",
    "    - In the original paper :\n",
    "    - $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads})W^{0} $  \n",
    "    where $head_{i} = \\text{ Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ \n",
    "    where $W_{i}^{k,q,v}$  are the matrices that we use for converting simple word embeddings to key,query and value vectors. \n",
    "    - Wait a 🦆ing minute ! what's $W^{0}$, I'll come back to this later.\n",
    "    - Note that their dimensions are as follow : \n",
    "        - Q : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - K : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - V : $\\textstyle{sequence\\_length} \\times \\textstyle{d_{model}}$\n",
    "        - $W_{i}^{Q} : \\textstyle{d_{model}} \\times \\textstyle{d_{key}}$\n",
    "        - $W_{i}^{K} : \\textstyle{d_{model}} \\times \\textstyle{d_{key}}$\n",
    "        - $W_{i}^{V} : \\textstyle{d_{model}} \\times \\textstyle{d_{value}}$\n",
    "    - wait a minute : How did this $d_{key} \\& d_{value}$ come into picture ? \n",
    "    - Just to add generality ! In the discussing above, we divided the matrix such that the the $d_{key} \\& d_{value}$ came out to be same. What if someone decided to take up different sizes for key-query and value matrices. \n",
    "    - Just in case, it's key-query and value, why not key , query and value ? beacuse they're supposed to be multiplied 😭 like this: \n",
    "    $QK^{T}$ so for them to multiply all along well, we'll need $d_{key}$ same in both key and query matrix. \n",
    "    Note that this difference in the dimensions of key query value is in the matrix that is used to generate the key query and value matrices.\n",
    "    - Note that we still want the $d_{key}$ to me a factor of $d_{model}$\n",
    "    - Now follow the flow : \n",
    "    $head_{i} = \\text{ Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) = \\textstyle{ softmax}(\\frac{\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}}}{\\textstyle\\sqrt{d_{k}}})VW_{i}^{V}$\n",
    "\n",
    "    $ QW_{i}^{Q} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{key}}) = (\\textstyle{sequence\\_length} \\times d_{key}) $\n",
    "\n",
    "    $ KW_{i}^{K} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{key}})  = (\\textstyle{sequence\\_length} \\times d_{key})$\n",
    "\n",
    "    $VW_{i}^{V} : (\\textstyle{sequence\\_length} \\times d_{model})  \\times (d_{model} \\times \\textstyle{d_{value}}) = (\\textstyle{sequence\\_length} \\times d_{value}) $\n",
    "\n",
    "\n",
    "    $\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}} : (\\textstyle{sequence\\_length} \\times \\textstyle{sequence\\_length})$\n",
    "\n",
    "    $\\textstyle{QW_{i}^{Q}(KW_{i}^{K})^{T}} :(\\textstyle{sequence\\_length} \\times {d_{value}}) $\n",
    "\n",
    "    - So the final shape of attention for a single head comes out to be : $(\\textstyle{sequence\\_length} \\times {d_{value}}) $\n",
    "\n",
    "    - Since $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads})$\n",
    "    $\\implies \\text{MULTIHEAD ATTENTION} = (\\textstyle{sequence\\_length} \\times (num\\_heads \\times d_{value})) $ \n",
    "    - Note that it's still a $2D$ Matrix, it's just that the columns of this matrix increased $num\\_heads$ times. \n",
    "    - Now here we have a smol 🤏 problem : \n",
    "        - the current shape is : $(\\textstyle{sequence\\_length} \\times (num\\_heads \\times d_{value}))$\n",
    "        - But for further steps we need the matrix in $(\\textstyle{sequence\\_length} \\times  d_{model})$\n",
    "        - How to convert ? 🤔💡\n",
    "        - Simple , multiply it by a matrix whose dimensions are $((num\\_heads \\times d_{value})\\times d_{model})$ which is nothing but $W^{0}$ as in the formula shown in the very beginning!\n",
    "        - That's why  $\\text{MULTIHEAD ATTENTION} = \\textstyle{Concat}(head_{1},head_{2},\\cdots,head_{num\\_heads}) \\times \\textbf W^{0}$\n",
    "        - This $W^0$ is important in this aspect. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some noteworthy implementation details : \n",
    "\n",
    "- we're going not gonna make num_heads number of different matrices man 😩! \n",
    "\n",
    "- That's not handy to maintain and will need some different logic for training 💀.I know I can't even implement a simple backpropagation 😔, implementing for a custom system of which I've got no clue of isn't very convincing to go with 🤐.   \n",
    "- What we're gonaa do is rather something that we consider a smart move but it's really not cause we're not that smart as we think of ourselves 😔💡: \n",
    "     - Make a single matrix for Key query values for all the attention heads. \n",
    "     - Fragment the final matrix of key query values into the desired number of heads and then calculate the attention for each head.\n",
    "     - After you're done with calcuating the attention of each head, we'll simlpy \n",
    "     concatenate that as we planned before. \n",
    "     - Something like this : \n",
    "    <div align=\"center\">\n",
    "\n",
    "    <img src=\"multihead_attention_implementation_for_key_qeury_value_matrices.jpeg\">\n",
    "\n",
    "    </div>\n",
    "\n",
    "    - Now one question : Wouldn't this kill  our very idea of multi-head attention 😨🥹?\n",
    "    - Isn't this simply calculating attention for once and not for the times the number of heads we have 😭? \n",
    "\n",
    "    - **Actually not!**\n",
    "    - We're seeing this in the wrong way!\n",
    "    - We're just postponing steps for our conveience. \n",
    "    - Instead of breaking the key query value matrix for each head, we're first calcuating the combined matrices for key query value and then break them down for calcuating attention.  \n",
    "```python\n",
    "import numpy as np \n",
    "word_vector = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "key = np.array([[1,2,3,4],\n",
    "                [1,2,3,4],\n",
    "                [1,2,3,4],\n",
    "                [1,2,3,4]\n",
    "            ])\n",
    "query = np.array([[1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4]\n",
    "            ])\n",
    "value = np.array([[1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4]\n",
    "            ])\n",
    "\n",
    "word_key = np.matmul(word_vector,key)\n",
    "word_query = np.matmul(word_vector,query)\n",
    "word_value = np.matmul(word_vector,value)\n",
    "\n",
    "word_key_part_1 = word_key[:,0:2]\n",
    "word_key_part_2 = word_key[:,2:4]\n",
    "\n",
    "word_query_part_1 = word_query[:,0:2]\n",
    "word_query_part_2 = word_query[:,2:4]\n",
    "\n",
    "word_value_part_1 = word_value[:,0:2]\n",
    "word_value_part_2 = word_value[:,2:4]\n",
    "\n",
    "\n",
    "attention_part1 = np.matmul(np.matmul(word_query_part_1,word_key_part_1.transpose())/np.sqrt(2),word_value_part_1)\n",
    "attention_part2 = np.matmul(np.matmul(word_query_part_2,word_key_part_2.transpose())/np.sqrt(2),word_value_part_2)\n",
    "\n",
    "key1 = np.array([[1,2,],\n",
    "                [1,2,],\n",
    "                [1,2,],\n",
    "                [1,2,]\n",
    "            ])\n",
    "query1 = np.array([[1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,]\n",
    "            ])\n",
    "value1 = np.array([[1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,],\n",
    "                  [1,2,]\n",
    "            ])\n",
    "\n",
    "key2 = np.array([[3,4,],\n",
    "                 [3,4,],\n",
    "                 [3,4,],\n",
    "                 [3,4,]\n",
    "            ])\n",
    "query2 = np.array([[3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,]\n",
    "            ])\n",
    "value2 = np.array([[3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,],\n",
    "                   [3,4,]\n",
    "            ])\n",
    "\n",
    "word_key1 = np.matmul(word_vector,key1)\n",
    "word_query1 = np.matmul(word_vector,query1)\n",
    "word_value1 = np.matmul(word_vector,value1)\n",
    "attention1 = np.matmul(np.matmul(word_query1,word_key1.transpose()/np.sqrt(2)),word_value1)\n",
    "\n",
    "\n",
    "word_key2 = np.matmul(word_vector,key2)\n",
    "word_query2 = np.matmul(word_vector,query2)\n",
    "word_value2 = np.matmul(word_vector,value2)\n",
    "attention2 = np.matmul(np.matmul(word_query2,word_key2.transpose()/np.sqrt(2)),word_value2)\n",
    "\n",
    "\n",
    "print(\"attention1 = \"attention1,)\n",
    "print(\"attention2 = \"attention2,)\n",
    "print(\"attention_part1 = \"attention_part1,)\n",
    "print(\"attention_part2 = \"attention_part2,)\n",
    "\n",
    "\n",
    "OUTPUTS : \n",
    "attention1 =      [[ 27435.74311004  54871.48622008]\n",
    "                   [ 71332.9320861  142665.8641722 ]]\n",
    "attention2 =      [[ 411536.14665057  548714.86220076]\n",
    "                   [1069993.98129148 1426658.64172198]]\n",
    "attention_part1 = [[ 27435.74311004  54871.48622008]\n",
    "                   [ 71332.9320861  142665.8641722 ]]\n",
    "attention_part2 = [[ 411536.14665057  548714.86220076]\n",
    "                   [1069993.98129148 1426658.64172198]]\n",
    "```\n",
    "- This is a small simulation of what we're doing here. \n",
    "- In the first case, we're calculating the attention without splitting the key query and value matrices. \n",
    "- In the second case we have splitted the key query value matrix and then we're calculating the attention. \n",
    "- In both the cases the correspoinding attention values came out to be same thus proving our point 😎. \n",
    "\n",
    "\n",
    "------\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A time for reality check ( yes, once again 😔)\n",
    "- Here above we said the we can use different dimensions for the key-query and value matrices but that's not quite true if we look it from the architectural point of view. \n",
    "- Here's why : \n",
    "    - Suppose we have $512$ dimensions for our word embeddings.\n",
    "    - Now we decide to take up $8$ attention heads. \n",
    "    - $\\implies d_{key} = 64$\n",
    "    - Now there are 8 key-value matrices. \n",
    "    - What about $d_{value}$?\n",
    "    - Consider that $d_{value} \\neq 64$\n",
    "    - This implies we'll have different number of value matrices which have some problems: \n",
    "        - How will we handle the $8$ resultant key query products ? \n",
    "        - With what value matrix we'll multiply the products since we don't have equal number of value matrices to multiply with the key-query product matrices. \n",
    "    - Hence for keeping our lives peaceful, we'll do $d_{key}=d_{value}=\\frac{d_{model}}{h}=\\frac{512}{8} = 64$  \n",
    "    - Now one important quesiton : What of the $W^{0}$ matrix ? Will we discard it off 😔? \n",
    "    - No, we'll keep it ( reason : TODO : FIND OUT ) \n",
    "\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOTHER CONCEPT : MASKING 🤡\n",
    "\n",
    "- While performing the operations on decoder, this is the general flow : \n",
    "- Suppose the sentence under consideration is : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**INPUT** \n",
    "|My | cat | is | a | beautiful | cat.| 😸 |\n",
    "|---|-----|----|---|-----------|-----|----|\n",
    "\n",
    "**OUTPUT**\n",
    "\n",
    "|मेरी | बिल्ली | एक | सुंदर | बिल्ली | है | 😸 |\n",
    "|----|------|----|------|------|---|-----|\n",
    "</div>\n",
    "\n",
    "- In the encoder it will be converted to a matrix of embeddings and positional embeddings. \n",
    "- The input will essentially look like this before entering the decoder : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    " (say $d_{model}=5$ )\n",
    "| My      |  $1$  |  $3$  |  $4$ |  $8$ |  $2$ |\n",
    "|---------|-------|-------|------|------|------|\n",
    "| cat     |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ | \n",
    "| is      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  a      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|beautiful|  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  cat    |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "- Now there are 2 scenarios which can be considered : \n",
    "     - Training \n",
    "     - Inference ( basically running the trained model for use )\n",
    "\n",
    "- Case 1 : **INFERENCE MODE**\n",
    "     - Here since the traditional method is to pass on the inputs of the previous decoder unit, there is no scope of parallelism. \n",
    "     - Something like this happens : \n",
    "     \n",
    "     <div align=\"center\">\n",
    "\n",
    "     <img src=\"inference_in_transformer.jpg\">\n",
    "\n",
    "     </div>\n",
    "\n",
    "     - So if we have say $100$ words, this has to take $100$ units of time given $1$ computation takes $1s$. \n",
    "\n",
    "\n",
    "- Case 2 : **TRANING MODE** : \n",
    "     - Here the story is a bit different. \n",
    "     - Here something similar happens but with a twist : \n",
    "          - Take the same translation example as above. \n",
    "          - Suppose instead of producing the first token as **मेरी**, the token produced was **हमारी**. \n",
    "          <div align=\"center\">\n",
    "\n",
    "          <img src=\"traninig_mode_in_transformer.jpg\"> \n",
    "\n",
    "          </div>\n",
    "\n",
    "          - Now here instead of passing **हमारी** in the next component of the decoder we pass on the True value that should have come at that place to train the model with fact that we want **बिल्ली** after the token **मेरी**. \n",
    "          - So the question arises : Do we need to actually know the **pervious state through the transformer** ? Or we simply know what's the next state gonna be ?\n",
    "          Also if know the next state , why to even perform the calculation using the transformer ? \n",
    "          - So the answer to the last question is **TRAINING🤡**. How the hell are we supposed to calculate the loss without knowing what the transformer knows and what we want it to know and make it learn. \n",
    "          - The answer to the second question is : Yes we do! from the training data itself. \n",
    "          - The answer is the first question is : No , we simply don't. Our sole purpose of using performing this operation is to train the transformer.\n",
    "          - Since we know what's gonna be the next state , can we parallelize this operation for a single sequence ? The answer is Yes we can and that's what we do. \n",
    "\n",
    "### SO EVERYTHING LOOKS FINE TILL NOW ? \n",
    "- One wise man said, if you think it going to end well, then you're not paying attention 💀.\n",
    "- Here there's one issue while training : \n",
    "- When we send the embeddings into the decoder parallely, what essentially we're doing is: \n",
    "      - Suppose we send the sentence : \n",
    "     <div align=\"center\">\n",
    "\n",
    "     |My | cat | is | a | beautiful | cat.| 😸 |\n",
    "     |---|-----|----|---|-----------|-----|----|\n",
    "\n",
    "     </div>\n",
    "\n",
    "- here when we sent the word **My**, we're essentially sending the information of what's going to come next in our sentence . \n",
    "- How so ? \n",
    "- Like this : \n",
    "- We wanted sent this input to the decoder : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "(say $d_{model}=5$ )\n",
    "\n",
    "| My      |  $1$  |  $3$  |  $4$ |  $8$ |  $2$ |\n",
    "|---------|-------|-------|------|------|------|\n",
    "| cat     |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ | \n",
    "| is      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  a      |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|beautiful|  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "|  cat    |  $1$  |  $5$  |  $4$ |  $5$ |  $6$ |\n",
    "           \n",
    "</div>\n",
    "\n",
    "- We sent the contextual embeddings ! \n",
    "- These emebeddings contain the information about the words that came before and **AFTER** the current word. \n",
    "- But how ? \n",
    "- Like this : \n",
    "- What we learned before was that we need to express the embedding of the current word in terms of it's own meaning and the words around it. \n",
    "<div align=\"center\">\n",
    "\n",
    "Say we're considering the word **beautiful**:\n",
    "\n",
    "$e_{beautiful} = \\alpha_{1}.e_{my} + \\alpha_{2}.e_{cat} + \\alpha_{3}.e_{is} + \\alpha_{4}.e_{a} + \\alpha_{5}.e_{beautiful} + \\alpha_{6}.e_{cat} $\n",
    "</div>\n",
    "\n",
    "- Till $\\alpha_{5}.e_{beautiful}$ was okay because till that point we had the context of the words that came before. But the moment the last embedding vector term came into picture  $ \\alpha_{6}.e_{cat} $ , it kind of gave off the hint to the transformer that the next word is cat which shouldn't happen. \n",
    "- Ideally we would want to **HIDE** this information that the word **cat** is going to come up because in that case it'll fail to learn to learn. \n",
    "- So what we want is that while processing the inputs, we keep the coefficients of the words that are comming after the current word to be zero. \n",
    "- That means : \n",
    "      - If our current word is Cat ( the first occurance ) then the values : \n",
    "      $\\alpha_{3} = \\alpha_{4} =  \\alpha_{5} =  \\alpha_{6} = 0$\n",
    "- So ideally speaking, this should be done at the step of self attention calculation.\n",
    "- There we calculated the coefficient matrix for multiplying it with the word embeddings. \n",
    "- $softmax(\\displaystyle\\frac{QK^{T}}{\\sqrt{d_{k}}})$ this step to be specific. \n",
    "- The output of this operation is actually the coefficients for the value matrix. \n",
    "- For better understanding, see this : \n",
    "<div align=\"center\">\n",
    "\n",
    "|          |My        |cat       |is        |a         |beautiful|cat       |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|My        |  1       |   5      |   6      |    6     |   3     |   5      |              \n",
    "|cat       |  2       |   5      |   6      |    6     |   3     |   5      |              \n",
    "|is        |  3       |   5      |   6      |    6     |   3     |   5      |    \n",
    "|a         |  4       |   5      |   6      |    6     |   3     |   5      |    \n",
    "|beautiful |  5       |   5      |   6      |    6     |   3     |   5      |         \n",
    "|cat       |  5       |   5      |   6      |    6     |   3     |   5      |              \n",
    "\n",
    "</div>\n",
    "\n",
    "- Here for the first row, the values in the row represent the coefficeints that are to be multiplied with the value vectors of the corresponding words . \n",
    "- Now here what we would like to have is something like this : \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|          |My        |cat       |is        |a         |beautiful|cat       |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|My        |  1       |   0      |   0      |    0     |   0     |   0      |              \n",
    "|cat       |  2       |   5      |   0      |    0     |   0     |   0      |              \n",
    "|is        |  3       |   5      |   6      |    0     |   0     |   0      |    \n",
    "|a         |  4       |   5      |   6      |    6     |   0     |   0      |    \n",
    "|beautiful |  5       |   5      |   6      |    6     |   3     |   0      |         \n",
    "|cat       |  5       |   5      |   6      |    6     |   3     |   5      |              \n",
    "\n",
    "</div>\n",
    "- So to achieve this what we do is : \n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"masking_demo.jpg\">\n",
    "\n",
    "</div>\n",
    "\n",
    "- Take a mask matrix and add it to the scaled matrix before performing softmax. \n",
    "- $softmax(-\\infty)=0$\n",
    "- This makes the coefficient of the future terms to be $0$\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD AND NORM \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views and contiguous in pytorch : [Reference Article](https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd)\n",
    "\n",
    "\n",
    "- The `.view` function in python does nothing much apart from returning the alternate way of **viewing** the data chunk\n",
    "\n",
    "- View is nothing but an alternative way to interpret the original tensor’s dimension without making a physical copy in the memory.\n",
    "\n",
    "- This means that any change in the view instance of the tensor will reflect in the orginial tensor as well since the view tensor is reading the data from the same memory address as of the original tensor.\n",
    "\n",
    "- Same goes for the fact when the orginal tensor is modified, the view tensor also gets changed for the very same reason. \n",
    "\n",
    "- Now certain operation in return a view and certain don't. \n",
    "- #TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model:int = 512,h:int = 8, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # expected input dimesions in each word\n",
    "        \n",
    "        self.h  = h   # number of heads \n",
    "        assert d_model%h == 0 ,\"Expected values of d_model and h such that d_model is divisible by h\"\n",
    "        self.d_key = d_model//h\n",
    "        self.w_query = nn.Linear(d_model,d_model) # weight matrix for query \n",
    "        self.w_key = nn.Linear(d_model,d_model) # weight matrix for key \n",
    "        \n",
    "        \n",
    "        # Note that here we're  going for d_model X d_model and not for d_model X (h*d_value)\n",
    "        # Since we're given an input matrix of d_model.  \n",
    "        self.w_value = nn.Linear(d_model,d_model) # weight matrix for value \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "    \n",
    "    \n",
    "    @staticmethod # this is a static method and can be accessed outside the class using this: MultiHeadAttention.attention(input parameters)\n",
    "    def attention(self,key:torch.Tensor,query:torch.Tensor,value:torch.Tensor,mask):\n",
    "        key_query = torch.matmul(query,key.T)\n",
    "        scaled = key_query/torch.sqrt(self.d_key)  \n",
    "        # At this point , the shape of the matrix is : (Batch , h , seqeuence_length , seqeuence_length)\n",
    "        if mask is not None:\n",
    "            scaled.masked_fill(mask==0,-1e9)\n",
    "            \n",
    "        softmax_ = scaled.softmax(dim=-1)\n",
    "        \n",
    "        return torch.matmul(softmax_,value)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self,query,key,value,mask):\n",
    "        q_w = self.w_query(query) # query matrix for input sequence , Shape : (Batch , sequence_length , d_model) \n",
    "        k_w = self.w_key(key)     # key matrix for input sequence   , Shape : (Batch , sequence_length , d_model)\n",
    "        v_w = self.w_value(value) # value matrix for input sequence , Shape : (Batch , sequence_length , d_model)\n",
    "        batch,sequence_length,d_model = q_w.shape\n",
    "        \n",
    "        # This is done to break the resultant query matrix of the sequence into h matrices\n",
    "        # transpose is mainly done to make the matrix head-based indexable. \n",
    "        q_q_head_wise = q_w.view((batch,sequence_length,self.h,self.d_key).transpose(2,1)) \n",
    "        q_k_head_wise = q_w.view((batch,sequence_length,self.h,self.d_key).transpose(2,1)) \n",
    "        q_v_head_wise = q_w.view((batch,sequence_length,self.h,self.d_key).transpose(2,1)) \n",
    "        attention_score = MultiHeadAttention.attention(self,q_k_head_wise,q_q_head_wise,q_v_head_wise,mask)\n",
    "        \n",
    "        #TODO : Study contiguous memory allocation in detail. \n",
    "        \n",
    "        attention_score = attention_score.transpose(1,2).contiguous().view(batch,sequence_length,self.h*self.d_key)\n",
    "        # Tanspose to get the dimension normally indexable and merging the tensors back. \n",
    "        \n",
    "        return self.w_o(attention_score)\n",
    "    \n",
    "    \n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,epsillon: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.epsillon = epsillon\n",
    "        self.aplha = nn.Parameter(torch.ones(size=1))\n",
    "        self.beta  = nn.Parameter(torch.ones(size=1))\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean = x.mean(dim = -1,keepdim=True) # x ---> (batch,row,col)\n",
    "        std  = x.std(dim = -1,keepdim=True) \n",
    "        normalized = self.aplha * (x-mean)/(std + self.epsillon) + self.beta\n",
    "        return normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, self_attention_block:MultiHeadAttention,feed_forward_block : ):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection : TODO😭\n",
    "- They are specifically placed at many places. \n",
    "- Study [this paper](https://arxiv.org/pdf/1603.05027) for better understanding of why residual connections workout well for large and deep models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.,   2.,   3.,   4.,   5.],\n",
      "         [  1.,   2.,   3.,   4.,   5.],\n",
      "         [  1.,   2.,   3.,   4.,   5.],\n",
      "         [  1.,   2.,   3.,   4.,   5.]],\n",
      "\n",
      "        [[ 10.,  20.,  30.,  40.,  50.],\n",
      "         [ 10.,  20.,  30.,  40.,  50.],\n",
      "         [ 10.,  20.,  30.,  40.,  50.],\n",
      "         [ 10.,  20.,  30.,  40.,  50.]],\n",
      "\n",
      "        [[100., 200., 300., 400., 500.],\n",
      "         [100., 200., 300., 400., 500.],\n",
      "         [100., 200., 300., 400., 500.],\n",
      "         [100., 200., 300., 400., 500.]]])\n",
      "----------------------\n",
      "tensor([[[  3.],\n",
      "         [  3.],\n",
      "         [  3.],\n",
      "         [  3.]],\n",
      "\n",
      "        [[ 30.],\n",
      "         [ 30.],\n",
      "         [ 30.],\n",
      "         [ 30.]],\n",
      "\n",
      "        [[300.],\n",
      "         [300.],\n",
      "         [300.],\n",
      "         [300.]]])\n",
      "----------------------\n",
      "tensor([[  3.,   3.,   3.,   3.],\n",
      "        [ 30.,  30.,  30.,  30.],\n",
      "        [300., 300., 300., 300.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "x = torch.tensor([\n",
    "                    [\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                        [1  ,2  ,3  ,4  ,5  ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                        [10 ,20 ,30 ,40 ,50 ],\n",
    "                    ],\n",
    "                    [\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                        [100,200,300,400,500],\n",
    "                    ],\n",
    "                ],\n",
    "                dtype=torch.float32)\n",
    "# print(\"x.shape = \",x.shape)\n",
    "# transpose = x.transpose(0,1)\n",
    "# print(\"transpose.shape = \",transpose.shape)\n",
    "# print(transpose)\n",
    "print(x)\n",
    "print(\"----------------------\")\n",
    "print(x.mean(dim=-1,keepdim=True))\n",
    "print(\"----------------------\")\n",
    "print(x.mean(dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
      "         [-0.7581,  1.0783,  0.8008,  1.6806],\n",
      "         [ 0.3559, -0.6866, -0.4934,  0.2415],\n",
      "         [-0.2316,  0.0418, -0.2516,  0.8599],\n",
      "         [-0.3097, -0.3957,  0.8034, -0.6216]]])\n",
      "tensor([[[ 0.8716,  0.5928,  0.2209, -1.6853],\n",
      "         [-1.6203,  0.4198,  0.1115,  1.0889],\n",
      "         [ 1.1111, -1.1985, -0.7703,  0.8577],\n",
      "         [-0.7452, -0.1393, -0.7894,  1.6739],\n",
      "         [-0.3243, -0.4803,  1.6947, -0.8900]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8211, 1.0394, 0.5212, 0.5210, 0.6366]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(42)\n",
    "batch, sentence_length, embedding_dim = 1, 5, 4\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "print(embedding)\n",
    "layer_norm_output = layer_norm(embedding)\n",
    "print(layer_norm_output)\n",
    "mean = embedding.sum(dim = 2)/embedding.shape[2]\n",
    "std = embedding.std(dim=2)\n",
    "std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1.], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0.], requires_grad=True))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer_norm.named_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
