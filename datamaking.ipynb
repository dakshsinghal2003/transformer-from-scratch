{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "df = pd.read_csv(\"test.tsv\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['en_query', 'cs_query'], dtype='object')\n",
      "170083\n",
      "Index(['en_query', 'cs_query'], dtype='object')\n",
      "6513\n",
      "Index(['en_query', 'cs_query'], dtype='object')\n",
      "2993\n",
      "Index(['en_query', 'cs_query'], dtype='object')\n",
      "1390\n"
     ]
    }
   ],
   "source": [
    "names_list= ['./synth-train.csv',\"./test.csv\",\"./train.csv\",\"./validation.csv\"]\n",
    "for data in names_list:\n",
    "    df = pd.read_csv(data)\n",
    "    print(df.columns)\n",
    "    name = data.replace(\"tsv\",\"csv\")\n",
    "    # df[['en_query','cs_query']].to_csv(name,index=False)\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 180979 examples [00:00, 537240.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(path = 'csv',data_files=  names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 180979/180979 [00:00<00:00, 2640896.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data.save_to_disk( \"./hinglish-to-english-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en_query', 'cs_query'],\n",
       "    num_rows: 180979\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "tokenizer = Tokenizer(model = WordPiece(unk_token=\"[UNK]\")).from_file(path = 'D:/Lectures/pytorch/Transformer from scratch/transformer-from-scratch/codes/py_files/tokenizer-en_query-.json') # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21964"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "##ais\n",
      "##e\n",
      "ho\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for data in  tokenizer.encode(\"kaise ho ?\").ids:\n",
    "    print(tokenizer.id_to_token(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
